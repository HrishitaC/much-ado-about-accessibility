{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General comparison: Control and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_comparison_desc_stats(merged_df, file_name):\n",
    "    markers = [\n",
    "            \"ratio_paras\",\n",
    "            \"ratio_list_items\",\n",
    "            \"ratio_headings\",\n",
    "            \"avg_para_len\",\n",
    "            \"num_sentences\",\n",
    "            \"avg_len\",\n",
    "            \"flesch\",\n",
    "            \"cli\",\n",
    "            \"avg_concrete\",\n",
    "            \"concrete_ratio\",\n",
    "            \"abstract_ratio\",\n",
    "            \"undefined_ratio\"\n",
    "        ]\n",
    "\n",
    "    response_group_types = [\n",
    "        \"SERP\",\n",
    "        \"RR\",\n",
    "        \"Chatbot\"\n",
    "    ]\n",
    "\n",
    "    response_groups = [\n",
    "        \"Google SERP\",\n",
    "        \"Google RR\",\n",
    "        \"Bing SERP\",\n",
    "        \"Bing RR\",\n",
    "        \"Gemini\",\n",
    "        \"GPT 3.5\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for response_group_type in response_group_types:\n",
    "        df = merged_df.loc[merged_df[\"response_group_type\"]==response_group_type]\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            results.append([response_group_type, marker, mean, std, median])\n",
    "\n",
    "    for response_group in response_groups:\n",
    "        df = merged_df.loc[merged_df[\"response_group\"]==response_group]\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            results.append([response_group, marker, mean, std, median])\n",
    "\n",
    "\n",
    "    desc_stats_gen_comparison = pd.DataFrame(results, columns=[\"group_name\", \"marker\", \"mean\", \"std\", \"median\"])\n",
    "    desc_stats_gen_comparison.to_excel(\"../stats/\" + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "\n",
    "target_merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt], ignore_index=True)\n",
    "\n",
    "\n",
    "bing_RR_control = pd.read_excel(\"../results/Control_Bing_RR_merged.xlsx\")\n",
    "bing_snip_control = pd.read_excel(\"../results/Control_Bing_SERP_merged.xlsx\")\n",
    "google_RR_control = pd.read_excel(\"../results/Control_Google_RR_merged.xlsx\")\n",
    "google_snip_control = pd.read_excel(\"../results/Control_Google_SERP_merged.xlsx\")\n",
    "gemini_control = pd.read_excel(\"../results/Control_gemini_merged.xlsx\")\n",
    "gpt_control = pd.read_excel(\"../results/Control_ChatGPT_merged.xlsx\")\n",
    "control_merged_df = pd.concat([google_snip_control, google_RR_control, bing_snip_control, bing_RR_control, gemini_control, gpt_control], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_comparison_desc_stats(target_merged_df, \"DescriptiveStatsGenComparison.xlsx\")\n",
    "gen_comparison_desc_stats(control_merged_df, \"Control_DescriptiveStatsGenComparison.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = target_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "response_groups = [\n",
    "    \"Google SERP\",\n",
    "    \"Google RR\",\n",
    "    \"Bing SERP\",\n",
    "    \"Bing RR\",\n",
    "    \"Gemini\",\n",
    "    \"GPT 3.5\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for response_group in response_groups:\n",
    "    for ngram in [\"1\", \"2\", \"3\", \"4\", \">4\"]:\n",
    "        df = merged_df.loc[(merged_df[\"response_group\"]==response_group) & (merged_df['ngram']==ngram)]\n",
    "        if df.empty:\n",
    "            print(ngram, response_group)\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            results.append([response_group, str(ngram) + \"-gram\", marker, mean, std, median])\n",
    "    \n",
    "    for domain in [True, False]:\n",
    "        df = merged_df.loc[(merged_df[\"response_group\"]==response_group) & (merged_df['domain_specific']==domain)]\n",
    "        if df.empty:\n",
    "            print(domain, response_group)\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            if domain == True:\n",
    "                category = \"domain specific\"\n",
    "            else:\n",
    "                category = \"general\"\n",
    "            results.append([response_group, category, marker, mean, std, median])\n",
    "\n",
    "\n",
    "desc_stats_query_category = pd.DataFrame(results, columns=[\"group_name\", \"query_category\", \"marker\", \"mean\", \"std\", \"median\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats_query_category.to_excel(\"../stats/DescriptiveStatsQueryCategory.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "google_reformed_RR = pd.read_excel(\"../results/Google_QueryReformed_RR_merged.xlsx\")\n",
    "google_reformed_snip = pd.read_excel(\"../results/Google_QueryReformed_SERP_merged.xlsx\")\n",
    "bing_reformed_RR = pd.read_excel(\"../results/Bing_QueryReformed_RR_merged.xlsx\")\n",
    "bing_reformed_snip = pd.read_excel(\"../results/Bing_QueryReformed_SERP_merged.xlsx\")\n",
    "gemini_reformed = pd.read_excel(\"../results/Gemini_QueryReformed_merged.xlsx\")\n",
    "gpt_reformed = pd.read_excel(\"../results/ChatGPT_QueryReformed_merged.xlsx\")\n",
    "\n",
    "merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt, google_reformed_snip, google_reformed_RR, bing_reformed_snip, bing_reformed_RR, gemini_reformed, gpt_reformed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "response_group_types = [\n",
    "    [\"SERP\", \"SERP Reformed\"],\n",
    "    [\"RR\", \"RR Reformed\"],\n",
    "    [\"Chatbot\", \"Chatbot Reformed\"],\n",
    "]\n",
    "\n",
    "response_groups = [\n",
    "    [\"Google SERP\", \"Google SERP Reformed\"],\n",
    "    [\"Google RR\", \"Google RR Reformed\"],\n",
    "    [\"Bing SERP\", \"Bing SERP Reformed\"],\n",
    "    [\"Bing RR\", \"Bing RR Reformed\"],\n",
    "    [\"Gemini\", \"Gemini Reformed\"],\n",
    "    [\"GPT 3.5\", \"GPT 3.5 Reformed\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for response_group_type in response_group_types:\n",
    "    original = response_group_type[0]\n",
    "    reformed = response_group_type[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group_type\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group_type\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_diff = round(np.mean(df_reformed[marker].to_numpy()) - np.mean(df_original[marker].to_numpy()), 2)\n",
    "        std_diff = round(np.std(df_reformed[marker].to_numpy()) - np.std(df_original[marker].to_numpy()), 2)\n",
    "        median_diff = round(np.median(df_reformed[marker].to_numpy()) - np.median(df_original[marker].to_numpy()), 2)\n",
    "        results.append([response_group_type[0], marker, mean_diff, std_diff, median_diff])\n",
    "\n",
    "for response_group in response_groups:\n",
    "    original = response_group[0]\n",
    "    reformed = response_group[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_original = np.mean(df_original[marker].to_numpy())\n",
    "        mean_reformed = np.mean(df_reformed[marker].to_numpy()) \n",
    "        std_original = np.std(df_original[marker].to_numpy())\n",
    "        std_reformed = np.std(df_reformed[marker].to_numpy())\n",
    "        median_original = np.median(df_original[marker].to_numpy())\n",
    "        median_reformed = np.median(df_reformed[marker].to_numpy())\n",
    "        results.append([response_group[0], marker, mean_original, std_original, median_original, mean_reformed, std_reformed, median_reformed])\n",
    "\n",
    "\n",
    "desc_stats_query_reformulation = pd.DataFrame(results, columns=[\"group_name\", \"marker\", \"mean_original\", \"std_original\", \"median_original\", \"mean_reformed\", \"std_reformed\", \"median_reformed\"])\n",
    "desc_stats_query_reformulation.to_excel(\"../stats/DescriptiveStatsQueryReformulation.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for response_group_type in response_group_types:\n",
    "    original = response_group_type[0]\n",
    "    reformed = response_group_type[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group_type\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group_type\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_diff = round(np.mean(df_reformed[marker].to_numpy()) - np.mean(df_original[marker].to_numpy()), 2)\n",
    "        std_diff = round(np.std(df_reformed[marker].to_numpy()) - np.std(df_original[marker].to_numpy()), 2)\n",
    "        median_diff = round(np.median(df_reformed[marker].to_numpy()) - np.median(df_original[marker].to_numpy()), 2)\n",
    "        results.append([response_group_type[0], marker, mean_diff, std_diff, median_diff])\n",
    "\n",
    "for response_group in response_groups:\n",
    "    original = response_group[0]\n",
    "    reformed = response_group[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_diff = round(np.mean(df_reformed[marker].to_numpy()) - np.mean(df_original[marker].to_numpy()), 2)\n",
    "        std_diff = round(np.std(df_reformed[marker].to_numpy()) - np.std(df_original[marker].to_numpy()), 2)\n",
    "        median_diff = round(np.median(df_reformed[marker].to_numpy()) - np.median(df_original[marker].to_numpy()), 2)\n",
    "        results.append([response_group[0], marker, mean_diff, std_diff, median_diff])\n",
    "\n",
    "\n",
    "desc_stats_query_reformulation = pd.DataFrame(results, columns=[\"group_name\", \"marker\", \"mean_diff\", \"std_diff\", \"median_diff\"])\n",
    "desc_stats_query_reformulation.to_excel(\"../stats/DescriptiveStatsQueryReformulation_Difference.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAS comparison for target user queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "\n",
    "merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt], ignore_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, shapiro, f_oneway, kruskal\n",
    "import pandas as pd\n",
    "\n",
    "def get_pval(x, x_groups, y):\n",
    "\n",
    "    if len(x_groups) == 2:\n",
    "            # shapiro_1 = shapiro(merged_df.loc[merged_df[x]==x_groups[0]][y])[1]\n",
    "            # shapiro_2 = shapiro(merged_df.loc[merged_df[x]==x_groups[1]][y])[1]\n",
    "            # if shapiro_1 < 0.05 or shapiro_2 < 0.05:\n",
    "            #     test_name = \"Mann Whitney U test\"\n",
    "            #     test_stat, p_val = mannwhitneyu(merged_df.loc[merged_df[x]==x_groups[0]][y], merged_df.loc[merged_df[x]==x_groups[1]][y])\n",
    "            # else:\n",
    "            test_name = \"T-Test\"\n",
    "            test_stat, p_val = ttest_ind(merged_df.loc[merged_df[x]==x_groups[0]][y], merged_df.loc[merged_df[x]==x_groups[1]][y])\n",
    "\n",
    "    elif len(x_groups)==3:\n",
    "        # shapiros = [shapiro(merged_df.loc[merged_df[x]==x_groups[i]][y])[1] < 0.05 for i in range(len(x_groups))]\n",
    "        x1, x2, x3 = (merged_df.loc[merged_df[x]==x_groups[i]][y] for i in range(len(x_groups)))\n",
    "        # if any(shapiros):\n",
    "        #     test_name = \"Kruskal Wallis H test\"\n",
    "        #     test_stat, p_val = kruskal(x1, x2, x3)\n",
    "        # else:\n",
    "        test_name = \"One-way F test\"\n",
    "        test_stat, p_val = f_oneway(x1, x2, x3)\n",
    "\n",
    "    else:\n",
    "        # shapiros = [shapiro(merged_df.loc[merged_df[x]==x_groups[i]][y])[1] < 0.05 for i in range(len(x_groups))]\n",
    "        x1, x2, x3, x4, x5, x6 = (merged_df.loc[merged_df[x]==x_groups[i]][y] for i in range(len(x_groups)))\n",
    "        # if any(shapiros):\n",
    "        #     test_name = \"Kruskal Wallis H test\"\n",
    "        #     test_stat, p_val = kruskal(x1, x2, x3)\n",
    "        # else:\n",
    "        test_name = \"One-way F test\"\n",
    "        test_stat, p_val = f_oneway(x1, x2, x3, x4, x5, x6)\n",
    "         \n",
    "\n",
    "    return test_name, test_stat, p_val\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "groups = [\n",
    "     ['response_group_type', 'Response group type', ['SERP', 'RR', 'Chatbot']],\n",
    "     ['response_group', 'SERP', ['Google SERP', 'Bing SERP']],\n",
    "     ['response_group', 'RR', ['Google RR', 'Bing RR']],\n",
    "     ['response_group', 'Chatbot', ['Gemini', 'GPT 3.5']],\n",
    "     ['response_group', 'IAS', [\"Google SERP\", \"Bing SERP\", \"Google RR\", \"Bing RR\", \"Gemini\", \"GPT 3.5\"]]\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for group_type, group_name, groups in groups:\n",
    "     for marker in markers:\n",
    "        test_name, test_stat, p_val = get_pval(x = group_type, x_groups=groups, y=marker)\n",
    "        results.append([group_name, marker, test_name, test_stat, p_val, bool(p_val < 0.05)])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Comparison', 'Marker', 'Significance test', 'Test stat', 'p-value', 'is significant'])\n",
    "results_df.to_excel(\"../stats/GenComparison_SignificanceTests.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR_control = pd.read_excel(\"../results/Control_Bing_RR_merged.xlsx\")\n",
    "bing_snip_control = pd.read_excel(\"../results/Control_Bing_SERP_merged.xlsx\")\n",
    "google_RR_control = pd.read_excel(\"../results/Control_Google_RR_merged.xlsx\")\n",
    "google_snip_control = pd.read_excel(\"../results/Control_Google_SERP_merged.xlsx\")\n",
    "gemini_control = pd.read_excel(\"../results/Control_gemini_merged.xlsx\")\n",
    "gpt_control = pd.read_excel(\"../results/Control_ChatGPT_merged.xlsx\")\n",
    "\n",
    "control_df = pd.concat([google_snip_control, google_RR_control, bing_snip_control, bing_RR_control, gemini_control, gpt_control], ignore_index=True)\n",
    "\n",
    "bing_RR_target = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip_target = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR_target = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip_target = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini_target = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt_target = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "\n",
    "target_df = pd.concat([google_snip_target, google_RR_target, bing_snip_target, bing_RR_target, gemini_target, gpt_target], ignore_index=True)\n",
    "\n",
    "control_df[\"user group\"] = [\"Control group\"]*len(control_df)\n",
    "target_df[\"user group\"] = [\"ASD group\"]*len(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, shapiro\n",
    "import pandas as pd\n",
    "\n",
    "def get_pval(resp_group, y):\n",
    "\n",
    "    # shapiro_1 = shapiro(control_df.loc[control_df[\"response_group\"]==resp_group][y])[1]\n",
    "    # shapiro_2 = shapiro(target_df.loc[target_df[\"response_group\"]==resp_group][y])[1]\n",
    "    # if shapiro_1 < 0.05 or shapiro_2 < 0.05:\n",
    "    #     test_name = \"Mann Whitney U test\"\n",
    "    #     test_stat, p_val = mannwhitneyu(control_df.loc[control_df[\"response_group\"]==resp_group][y], target_df.loc[target_df[\"response_group\"]==resp_group][y])\n",
    "    # else:\n",
    "    test_name = \"T-Test\"\n",
    "    test_stat, p_val = ttest_ind(control_df.loc[control_df[\"response_group\"]==resp_group][y], target_df.loc[target_df[\"response_group\"]==resp_group][y])\n",
    "\n",
    "    return test_name, test_stat, p_val\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "groups = [\n",
    "     'Google SERP', 'Bing SERP', 'Google RR', 'Bing RR', 'Gemini', 'GPT 3.5'\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for group_type in groups:\n",
    "     for marker in markers:\n",
    "        test_name, test_stat, p_val = get_pval(resp_group=group_type, y=marker)\n",
    "        results.append([group_type, marker, test_name, test_stat, p_val, bool(p_val < 0.05)])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Comparison', 'Marker', 'Significance test', 'Test stat', 'p-value', 'is significant'])\n",
    "results_df.to_excel(\"../stats/ControlvsTarget_GenComparison_SignificanceTests_new.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt], ignore_index=True)\n",
    "# merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR], ignore_index=True)\n",
    "merged_df[\"ngram_raw\"] = [len(row[\"query\"].split(\" \")) for _,row in merged_df.iterrows()]\n",
    "merged_df[\"ngram_raw\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [group, category_type, marker]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hrishitachakra\\AppData\\Local\\miniconda3\\envs\\SOLandChildren\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:531: ConstantInputWarning: Each of the input arrays is constant; the F statistic is not defined or infinite\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal, mannwhitneyu, linregress, ttest_ind, f_oneway \n",
    "import numpy as np\n",
    "\n",
    "markers = [\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"ratio_headings\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_paras\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "    ]\n",
    "\n",
    "marker_name = {\n",
    "        \"ratio_paras\": \"ParaSents\",\n",
    "        \"ratio_list_items\": \"ListSents\",\n",
    "        \"ratio_headings\": \"HeadingSents\",\n",
    "        \"avg_para_len\": \"ParaLen\",\n",
    "        \"num_sentences\": \"SC\",\n",
    "        \"avg_len\": \"SentLen\",\n",
    "        \"flesch\": \"FRES\",\n",
    "        \"cli\": \"CLI\",\n",
    "        \"avg_concrete\": \"Conc\",\n",
    "        \"concrete_ratio\": \"ConcreteWords\",\n",
    "        \"abstract_ratio\": \"AbstractWords\"\n",
    "}\n",
    "\n",
    "groups = [\n",
    "    \"Google SERP\",\n",
    "    \"Google RR\",\n",
    "    \"Bing SERP\",\n",
    "    \"Bing RR\",\n",
    "    \"Gemini\",\n",
    "    \"GPT 3.5\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "issues = []\n",
    "\n",
    "for group in groups:\n",
    "    group_df = merged_df.loc[merged_df[\"response_group\"]==group]\n",
    "    for marker in markers:\n",
    "        # ngram\n",
    "        try:\n",
    "            test_stat, p_val = f_oneway(group_df.loc[group_df[\"ngram\"]=='1'][marker], group_df.loc[group_df[\"ngram\"]=='2'][marker], group_df.loc[group_df[\"ngram\"]=='3'][marker], group_df.loc[group_df[\"ngram\"]=='4'][marker], group_df.loc[group_df[\"ngram\"]=='>4'][marker])\n",
    "            if p_val < 0.05:\n",
    "                is_significant = 1\n",
    "            else:\n",
    "                is_significant = 0\n",
    "            results.append([group, \"ngram\", marker_name[marker], \"ANOVA\", test_stat, p_val, is_significant])\n",
    "        except:\n",
    "            issues.append([group, \"ngram-ANOVA\", marker_name[marker]])\n",
    "        try:\n",
    "            slope, intercept, r_val, p_val, se = linregress(group_df['ngram_raw'], group_df[marker])\n",
    "            test_name = \"Linear Regression\"\n",
    "            if r_val == None:\n",
    "                issues.append([group, \"ngram-LR\", marker_name[marker]])\n",
    "            elif p_val < 0.05:\n",
    "                is_significant = 1\n",
    "            else:\n",
    "                is_significant = 0\n",
    "            results.append([group, \"ngram\", marker_name[marker], \"correlation_coeff\", r_val, p_val, is_significant])\n",
    "            results.append([group, \"ngram\", marker_name[marker], \"regression_coeffs\", [slope,intercept], p_val, is_significant])\n",
    "        except:\n",
    "            issues.append([group, \"ngram-LR\", marker_name[marker]])\n",
    "\n",
    "        # domain specific\n",
    "        domain_specific = group_df.loc[group_df[\"domain_specific\"]==True][marker]\n",
    "        domain_general = group_df.loc[group_df[\"domain_specific\"]==False][marker]\n",
    "        try:\n",
    "            t_val, p_val = ttest_ind(domain_specific, domain_general)\n",
    "            if p_val < 0.05:\n",
    "                is_significant = 1\n",
    "            else:\n",
    "                is_significant = 0\n",
    "            test_name = \"T-Test\"\n",
    "            results.append([group, \"domain_specific\", marker_name[marker], test_name, t_val, p_val, is_significant])\n",
    "        except:\n",
    "            issues.append([group, \"domain_specific\", marker_name[marker]])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns= [\"group\", \"category_type\", \"marker\", \"test_name\", \"test_stats\", \"p_val\", \"is_significant\"])\n",
    "issues_df = pd.DataFrame(issues, columns=[\"group\", \"category_type\", \"marker\"])\n",
    "# print(results_df.head())\n",
    "# print()\n",
    "print(issues_df)\n",
    "# print()\n",
    "# print(len(results_df), len(results_df.loc[results_df[\"p_val\"]<=0.01]))\n",
    "results_df.to_excel(\"../stats/QueryCategory_SignificanceTests.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ngram</th>\n",
       "      <th>domain_specific</th>\n",
       "      <th>ratio_paras</th>\n",
       "      <th>ratio_list_items</th>\n",
       "      <th>ratio_headings</th>\n",
       "      <th>avg_para_len</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_len</th>\n",
       "      <th>flesch</th>\n",
       "      <th>cli</th>\n",
       "      <th>avg_concrete</th>\n",
       "      <th>concrete_ratio</th>\n",
       "      <th>abstract_ratio</th>\n",
       "      <th>undefined_ratio</th>\n",
       "      <th>response_group</th>\n",
       "      <th>response_group_type</th>\n",
       "      <th>Query Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adhd</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>16.00</td>\n",
       "      <td>3.71</td>\n",
       "      <td>9.87</td>\n",
       "      <td>28.06</td>\n",
       "      <td>16.30</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.56</td>\n",
       "      <td>Google SERP</td>\n",
       "      <td>SERP</td>\n",
       "      <td>Original Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adult</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>13.22</td>\n",
       "      <td>3.00</td>\n",
       "      <td>9.43</td>\n",
       "      <td>65.69</td>\n",
       "      <td>10.67</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.58</td>\n",
       "      <td>Google SERP</td>\n",
       "      <td>SERP</td>\n",
       "      <td>Original Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>16.19</td>\n",
       "      <td>4.12</td>\n",
       "      <td>9.08</td>\n",
       "      <td>44.93</td>\n",
       "      <td>13.36</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.53</td>\n",
       "      <td>Google SERP</td>\n",
       "      <td>SERP</td>\n",
       "      <td>Original Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>15.94</td>\n",
       "      <td>4.33</td>\n",
       "      <td>7.83</td>\n",
       "      <td>42.54</td>\n",
       "      <td>12.88</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>Google SERP</td>\n",
       "      <td>SERP</td>\n",
       "      <td>Original Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asd</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2.88</td>\n",
       "      <td>10.16</td>\n",
       "      <td>46.14</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.59</td>\n",
       "      <td>Google SERP</td>\n",
       "      <td>SERP</td>\n",
       "      <td>Original Query</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     query ngram  domain_specific  ratio_paras  ratio_list_items  \\\n",
       "0     adhd     1             True         0.68              0.00   \n",
       "1    adult     1            False         0.64              0.00   \n",
       "2      age     1            False         0.70              0.00   \n",
       "3  anxiety     1            False         0.75              0.00   \n",
       "4      asd     1            False         0.59              0.02   \n",
       "\n",
       "   ratio_headings  avg_para_len  num_sentences  avg_len  flesch    cli  \\\n",
       "0            0.32         16.00           3.71     9.87   28.06  16.30   \n",
       "1            0.36         13.22           3.00     9.43   65.69  10.67   \n",
       "2            0.30         16.19           4.12     9.08   44.93  13.36   \n",
       "3            0.25         15.94           4.33     7.83   42.54  12.88   \n",
       "4            0.39         13.00           2.88    10.16   46.14  15.00   \n",
       "\n",
       "   avg_concrete  concrete_ratio  abstract_ratio  undefined_ratio  \\\n",
       "0          2.32            0.03            0.41             0.56   \n",
       "1          2.43            0.07            0.35             0.58   \n",
       "2          2.39            0.02            0.45             0.53   \n",
       "3          2.19            0.02            0.49             0.49   \n",
       "4          2.14            0.01            0.40             0.59   \n",
       "\n",
       "  response_group response_group_type      Query Type  \n",
       "0    Google SERP                SERP  Original Query  \n",
       "1    Google SERP                SERP  Original Query  \n",
       "2    Google SERP                SERP  Original Query  \n",
       "3    Google SERP                SERP  Original Query  \n",
       "4    Google SERP                SERP  Original Query  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_RR[\"Query Type\"] = [\"Original Query\"]*len(bing_RR)\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "bing_snip[\"Query Type\"] = [\"Original Query\"]*len(bing_RR)\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_RR[\"Query Type\"] = [\"Original Query\"]*len(bing_RR)\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "google_snip[\"Query Type\"] = [\"Original Query\"]*len(bing_RR)\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gemini[\"Query Type\"] = [\"Original Query\"]*len(bing_RR)\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "gpt[\"Query Type\"] = [\"Original Query\"]*len(bing_RR)\n",
    "\n",
    "google_reformed_RR = pd.read_excel(\"../results/Google_QueryReformed_RR_merged.xlsx\")\n",
    "google_reformed_RR[\"Query Type\"] = [\"Reformed Query\"]*len(bing_RR)\n",
    "google_reformed_snip = pd.read_excel(\"../results/Google_QueryReformed_SERP_merged.xlsx\")\n",
    "google_reformed_snip[\"Query Type\"] = [\"Reformed Query\"]*len(bing_RR)\n",
    "bing_reformed_RR = pd.read_excel(\"../results/Bing_QueryReformed_RR_merged.xlsx\")\n",
    "bing_reformed_RR[\"Query Type\"] = [\"Reformed Query\"]*len(bing_RR)\n",
    "bing_reformed_snip = pd.read_excel(\"../results/Bing_QueryReformed_SERP_merged.xlsx\")\n",
    "bing_reformed_snip[\"Query Type\"] = [\"Reformed Query\"]*len(bing_RR)\n",
    "gemini_reformed = pd.read_excel(\"../results/Gemini_QueryReformed_merged.xlsx\")\n",
    "gemini_reformed[\"Query Type\"] = [\"Reformed Query\"]*len(bing_RR)\n",
    "gpt_reformed = pd.read_excel(\"../results/ChatGPT_QueryReformed_merged.xlsx\")\n",
    "gpt_reformed[\"Query Type\"] = [\"Reformed Query\"]*len(bing_RR)\n",
    "\n",
    "merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt, google_reformed_snip, google_reformed_RR, bing_reformed_snip, bing_reformed_RR, gemini_reformed, gpt_reformed], ignore_index=True)\n",
    "# merged_df[\"response_group\"] = [re.sub(\" Reformed\", \"\", response_group_name) for response_group_name in merged_df[\"response_group\"]]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, shapiro, ttest_ind\n",
    "\n",
    "def get_pval_pairwise(x, x_groups, y):\n",
    "    test_name = \"T-Test\"\n",
    "    test_stat, p_val = ttest_ind(merged_df.loc[merged_df[x]==x_groups[0]][y], merged_df.loc[merged_df[x]==x_groups[1]][y])\n",
    "    return test_name, test_stat, p_val\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "groups_list = [\n",
    "    ['response_group_type', 'SERP', ['SERP', 'SERP Reformed']],\n",
    "    ['response_group_type', 'RR', ['RR', 'RR Reformed']],\n",
    "    ['response_group_type', 'Chatbot', ['Chatbot', 'Chatbot Reformed']],\n",
    "    ['response_group', 'Google SERP', ['Google SERP', 'Google SERP Reformed']],\n",
    "    ['response_group', 'Google RR',  ['Google RR', 'Google RR Reformed']],\n",
    "    ['response_group' , 'Bing SERP', ['Bing SERP', 'Bing SERP Reformed']],\n",
    "    ['response_group', 'Bing RR', ['Bing RR', 'Bing RR Reformed']],\n",
    "    ['response_group', 'Gemini', ['Gemini', 'Gemini Reformed']],\n",
    "    ['response_group', 'GPT 3.5', ['GPT 3.5', 'GPT 3.5 Reformed']]\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for group_type, group_name, groups in groups_list:\n",
    "    for marker in markers:\n",
    "        test_name, test_stat, p_val = get_pval_pairwise(x=group_type, x_groups=groups, y=marker)\n",
    "        results.append([group_name, marker, test_name, test_stat, p_val, bool(p_val < 0.05)])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Group name', 'marker', 'test name', 'test stat', 'p-val', 'is significant'])\n",
    "results_df.to_excel(\"../stats/QueryReformulation_SiginificanceTests.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group name</th>\n",
       "      <th>marker</th>\n",
       "      <th>test name</th>\n",
       "      <th>test stat</th>\n",
       "      <th>p-val</th>\n",
       "      <th>is significant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SERP</td>\n",
       "      <td>ratio_paras</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>-16.196735</td>\n",
       "      <td>1.462876e-52</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SERP</td>\n",
       "      <td>ratio_list_items</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>-2.119795</td>\n",
       "      <td>3.426968e-02</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERP</td>\n",
       "      <td>ratio_headings</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>17.969089</td>\n",
       "      <td>9.137391e-63</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SERP</td>\n",
       "      <td>avg_para_len</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>-8.080836</td>\n",
       "      <td>1.847746e-15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SERP</td>\n",
       "      <td>num_sentences</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>-15.915340</td>\n",
       "      <td>5.377182e-51</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>cli</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>-1.161452</td>\n",
       "      <td>2.460144e-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>avg_concrete</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>1.609151</td>\n",
       "      <td>1.082169e-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>concrete_ratio</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>0.621569</td>\n",
       "      <td>5.345096e-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>abstract_ratio</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>-2.815006</td>\n",
       "      <td>5.070858e-03</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>undefined_ratio</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>2.669648</td>\n",
       "      <td>7.841094e-03</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Group name            marker test name  test stat         p-val  \\\n",
       "0         SERP       ratio_paras    T-Test -16.196735  1.462876e-52   \n",
       "1         SERP  ratio_list_items    T-Test  -2.119795  3.426968e-02   \n",
       "2         SERP    ratio_headings    T-Test  17.969089  9.137391e-63   \n",
       "3         SERP      avg_para_len    T-Test  -8.080836  1.847746e-15   \n",
       "4         SERP     num_sentences    T-Test -15.915340  5.377182e-51   \n",
       "..         ...               ...       ...        ...           ...   \n",
       "103    GPT 3.5               cli    T-Test  -1.161452  2.460144e-01   \n",
       "104    GPT 3.5      avg_concrete    T-Test   1.609151  1.082169e-01   \n",
       "105    GPT 3.5    concrete_ratio    T-Test   0.621569  5.345096e-01   \n",
       "106    GPT 3.5    abstract_ratio    T-Test  -2.815006  5.070858e-03   \n",
       "107    GPT 3.5   undefined_ratio    T-Test   2.669648  7.841094e-03   \n",
       "\n",
       "     is significant  \n",
       "0              True  \n",
       "1              True  \n",
       "2              True  \n",
       "3              True  \n",
       "4              True  \n",
       "..              ...  \n",
       "103           False  \n",
       "104           False  \n",
       "105           False  \n",
       "106            True  \n",
       "107            True  \n",
       "\n",
       "[108 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOLandChildren",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
