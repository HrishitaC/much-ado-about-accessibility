{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General comparison: Control and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_comparison_desc_stats(merged_df, file_name):\n",
    "    markers = [\n",
    "            \"ratio_paras\",\n",
    "            \"ratio_list_items\",\n",
    "            \"ratio_headings\",\n",
    "            \"avg_para_len\",\n",
    "            \"num_sentences\",\n",
    "            \"avg_len\",\n",
    "            \"flesch\",\n",
    "            \"cli\",\n",
    "            \"avg_concrete\",\n",
    "            \"concrete_ratio\",\n",
    "            \"abstract_ratio\",\n",
    "            \"undefined_ratio\"\n",
    "        ]\n",
    "\n",
    "    response_group_types = [\n",
    "        \"SERP\",\n",
    "        \"RR\",\n",
    "        \"Chatbot\"\n",
    "    ]\n",
    "\n",
    "    response_groups = [\n",
    "        \"Google SERP\",\n",
    "        \"Google RR\",\n",
    "        \"Bing SERP\",\n",
    "        \"Bing RR\",\n",
    "        \"Gemini\",\n",
    "        \"GPT 3.5\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for response_group_type in response_group_types:\n",
    "        df = merged_df.loc[merged_df[\"response_group_type\"]==response_group_type]\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            results.append([response_group_type, marker, mean, std, median])\n",
    "\n",
    "    for response_group in response_groups:\n",
    "        df = merged_df.loc[merged_df[\"response_group\"]==response_group]\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            results.append([response_group, marker, mean, std, median])\n",
    "\n",
    "\n",
    "    desc_stats_gen_comparison = pd.DataFrame(results, columns=[\"group_name\", \"marker\", \"mean\", \"std\", \"median\"])\n",
    "    desc_stats_gen_comparison.to_excel(\"../stats/\" + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "\n",
    "target_merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt], ignore_index=True)\n",
    "\n",
    "\n",
    "bing_RR_control = pd.read_excel(\"../results/Control_Bing_RR_merged.xlsx\")\n",
    "bing_snip_control = pd.read_excel(\"../results/Control_Bing_SERP_merged.xlsx\")\n",
    "google_RR_control = pd.read_excel(\"../results/Control_Google_RR_merged.xlsx\")\n",
    "google_snip_control = pd.read_excel(\"../results/Control_Google_SERP_merged.xlsx\")\n",
    "gemini_control = pd.read_excel(\"../results/Control_gemini_merged.xlsx\")\n",
    "gpt_control = pd.read_excel(\"../results/Control_ChatGPT_merged.xlsx\")\n",
    "control_merged_df = pd.concat([google_snip_control, google_RR_control, bing_snip_control, bing_RR_control, gemini_control, gpt_control], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_comparison_desc_stats(target_merged_df, \"DescriptiveStatsGenComparison.xlsx\")\n",
    "gen_comparison_desc_stats(control_merged_df, \"Control_DescriptiveStatsGenComparison.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = target_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "response_groups = [\n",
    "    \"Google SERP\",\n",
    "    \"Google RR\",\n",
    "    \"Bing SERP\",\n",
    "    \"Bing RR\",\n",
    "    \"Gemini\",\n",
    "    \"GPT 3.5\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for response_group in response_groups:\n",
    "    for ngram in [\"1\", \"2\", \"3\", \"4\", \">4\"]:\n",
    "        df = merged_df.loc[(merged_df[\"response_group\"]==response_group) & (merged_df['ngram']==ngram)]\n",
    "        if df.empty:\n",
    "            print(ngram, response_group)\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            results.append([response_group, str(ngram) + \"-gram\", marker, mean, std, median])\n",
    "    \n",
    "    for domain in [True, False]:\n",
    "        df = merged_df.loc[(merged_df[\"response_group\"]==response_group) & (merged_df['domain_specific']==domain)]\n",
    "        if df.empty:\n",
    "            print(domain, response_group)\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            if domain == True:\n",
    "                category = \"domain specific\"\n",
    "            else:\n",
    "                category = \"general\"\n",
    "            results.append([response_group, category, marker, mean, std, median])\n",
    "\n",
    "\n",
    "desc_stats_query_category = pd.DataFrame(results, columns=[\"group_name\", \"query_category\", \"marker\", \"mean\", \"std\", \"median\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats_query_category.to_excel(\"../stats/DescriptiveStatsQueryCategory.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "google_reformed_RR = pd.read_excel(\"../results/Google_QueryReformed_RR_merged.xlsx\")\n",
    "google_reformed_snip = pd.read_excel(\"../results/Google_QueryReformed_SERP_merged.xlsx\")\n",
    "bing_reformed_RR = pd.read_excel(\"../results/Bing_QueryReformed_RR_merged.xlsx\")\n",
    "bing_reformed_snip = pd.read_excel(\"../results/Bing_QueryReformed_SERP_merged.xlsx\")\n",
    "gemini_reformed = pd.read_excel(\"../results/Gemini_QueryReformed_merged.xlsx\")\n",
    "gpt_reformed = pd.read_excel(\"../results/ChatGPT_QueryReformed_merged.xlsx\")\n",
    "\n",
    "merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt, google_reformed_snip, google_reformed_RR, bing_reformed_snip, bing_reformed_RR, gemini_reformed, gpt_reformed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "response_group_types = [\n",
    "    [\"SERP\", \"SERP Reformed\"],\n",
    "    [\"RR\", \"RR Reformed\"],\n",
    "    [\"Chatbot\", \"Chatbot Reformed\"],\n",
    "]\n",
    "\n",
    "response_groups = [\n",
    "    [\"Google SERP\", \"Google SERP Reformed\"],\n",
    "    [\"Google RR\", \"Google RR Reformed\"],\n",
    "    [\"Bing SERP\", \"Bing SERP Reformed\"],\n",
    "    [\"Bing RR\", \"Bing RR Reformed\"],\n",
    "    [\"Gemini\", \"Gemini Reformed\"],\n",
    "    [\"GPT 3.5\", \"GPT 3.5 Reformed\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for response_group_type in response_group_types:\n",
    "    original = response_group_type[0]\n",
    "    reformed = response_group_type[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group_type\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group_type\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_diff = round(np.mean(df_reformed[marker].to_numpy()) - np.mean(df_original[marker].to_numpy()), 2)\n",
    "        std_diff = round(np.std(df_reformed[marker].to_numpy()) - np.std(df_original[marker].to_numpy()), 2)\n",
    "        median_diff = round(np.median(df_reformed[marker].to_numpy()) - np.median(df_original[marker].to_numpy()), 2)\n",
    "        results.append([response_group_type[0], marker, mean_diff, std_diff, median_diff])\n",
    "\n",
    "for response_group in response_groups:\n",
    "    original = response_group[0]\n",
    "    reformed = response_group[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_original = np.mean(df_original[marker].to_numpy())\n",
    "        mean_reformed = np.mean(df_reformed[marker].to_numpy()) \n",
    "        std_original = np.std(df_original[marker].to_numpy())\n",
    "        std_reformed = np.std(df_reformed[marker].to_numpy())\n",
    "        median_original = np.median(df_original[marker].to_numpy())\n",
    "        median_reformed = np.median(df_reformed[marker].to_numpy())\n",
    "        results.append([response_group[0], marker, mean_original, std_original, median_original, mean_reformed, std_reformed, median_reformed])\n",
    "\n",
    "\n",
    "desc_stats_query_reformulation = pd.DataFrame(results, columns=[\"group_name\", \"marker\", \"mean_original\", \"std_original\", \"median_original\", \"mean_reformed\", \"std_reformed\", \"median_reformed\"])\n",
    "desc_stats_query_reformulation.to_excel(\"../stats/DescriptiveStatsQueryReformulation.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for response_group_type in response_group_types:\n",
    "    original = response_group_type[0]\n",
    "    reformed = response_group_type[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group_type\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group_type\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_diff = round(np.mean(df_reformed[marker].to_numpy()) - np.mean(df_original[marker].to_numpy()), 2)\n",
    "        std_diff = round(np.std(df_reformed[marker].to_numpy()) - np.std(df_original[marker].to_numpy()), 2)\n",
    "        median_diff = round(np.median(df_reformed[marker].to_numpy()) - np.median(df_original[marker].to_numpy()), 2)\n",
    "        results.append([response_group_type[0], marker, mean_diff, std_diff, median_diff])\n",
    "\n",
    "for response_group in response_groups:\n",
    "    original = response_group[0]\n",
    "    reformed = response_group[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_diff = round(np.mean(df_reformed[marker].to_numpy()) - np.mean(df_original[marker].to_numpy()), 2)\n",
    "        std_diff = round(np.std(df_reformed[marker].to_numpy()) - np.std(df_original[marker].to_numpy()), 2)\n",
    "        median_diff = round(np.median(df_reformed[marker].to_numpy()) - np.median(df_original[marker].to_numpy()), 2)\n",
    "        results.append([response_group[0], marker, mean_diff, std_diff, median_diff])\n",
    "\n",
    "\n",
    "desc_stats_query_reformulation = pd.DataFrame(results, columns=[\"group_name\", \"marker\", \"mean_diff\", \"std_diff\", \"median_diff\"])\n",
    "desc_stats_query_reformulation.to_excel(\"../stats/DescriptiveStatsQueryReformulation_Difference.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAS comparison for target user queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "\n",
    "merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt], ignore_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, shapiro, f_oneway, kruskal\n",
    "import pandas as pd\n",
    "\n",
    "def get_pval(x, x_groups, y):\n",
    "\n",
    "    if len(x_groups) == 2:\n",
    "            # shapiro_1 = shapiro(merged_df.loc[merged_df[x]==x_groups[0]][y])[1]\n",
    "            # shapiro_2 = shapiro(merged_df.loc[merged_df[x]==x_groups[1]][y])[1]\n",
    "            # if shapiro_1 < 0.05 or shapiro_2 < 0.05:\n",
    "            #     test_name = \"Mann Whitney U test\"\n",
    "            #     test_stat, p_val = mannwhitneyu(merged_df.loc[merged_df[x]==x_groups[0]][y], merged_df.loc[merged_df[x]==x_groups[1]][y])\n",
    "            # else:\n",
    "            test_name = \"T-Test\"\n",
    "            test_stat, p_val = ttest_ind(merged_df.loc[merged_df[x]==x_groups[0]][y], merged_df.loc[merged_df[x]==x_groups[1]][y])\n",
    "\n",
    "    elif len(x_groups)==3:\n",
    "        # shapiros = [shapiro(merged_df.loc[merged_df[x]==x_groups[i]][y])[1] < 0.05 for i in range(len(x_groups))]\n",
    "        x1, x2, x3 = (merged_df.loc[merged_df[x]==x_groups[i]][y] for i in range(len(x_groups)))\n",
    "        # if any(shapiros):\n",
    "        #     test_name = \"Kruskal Wallis H test\"\n",
    "        #     test_stat, p_val = kruskal(x1, x2, x3)\n",
    "        # else:\n",
    "        test_name = \"One-way F test\"\n",
    "        test_stat, p_val = f_oneway(x1, x2, x3)\n",
    "\n",
    "    else:\n",
    "        # shapiros = [shapiro(merged_df.loc[merged_df[x]==x_groups[i]][y])[1] < 0.05 for i in range(len(x_groups))]\n",
    "        x1, x2, x3, x4, x5, x6 = (merged_df.loc[merged_df[x]==x_groups[i]][y] for i in range(len(x_groups)))\n",
    "        # if any(shapiros):\n",
    "        #     test_name = \"Kruskal Wallis H test\"\n",
    "        #     test_stat, p_val = kruskal(x1, x2, x3)\n",
    "        # else:\n",
    "        test_name = \"One-way F test\"\n",
    "        test_stat, p_val = f_oneway(x1, x2, x3, x4, x5, x6)\n",
    "         \n",
    "\n",
    "    return test_name, test_stat, p_val\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "groups = [\n",
    "     ['response_group_type', 'Response group type', ['SERP', 'RR', 'Chatbot']],\n",
    "     ['response_group', 'SERP', ['Google SERP', 'Bing SERP']],\n",
    "     ['response_group', 'RR', ['Google RR', 'Bing RR']],\n",
    "     ['response_group', 'Chatbot', ['Gemini', 'GPT 3.5']],\n",
    "     ['response_group', 'IAS', [\"Google SERP\", \"Bing SERP\", \"Google RR\", \"Bing RR\", \"Gemini\", \"GPT 3.5\"]]\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for group_type, group_name, groups in groups:\n",
    "     for marker in markers:\n",
    "        test_name, test_stat, p_val = get_pval(x = group_type, x_groups=groups, y=marker)\n",
    "        results.append([group_name, marker, test_name, test_stat, p_val, bool(p_val < 0.05)])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Comparison', 'Marker', 'Significance test', 'Test stat', 'p-value', 'is significant'])\n",
    "results_df.to_excel(\"../stats/GenComparison_SignificanceTests.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR_control = pd.read_excel(\"../results/Control_Bing_RR_merged.xlsx\")\n",
    "bing_snip_control = pd.read_excel(\"../results/Control_Bing_SERP_merged.xlsx\")\n",
    "google_RR_control = pd.read_excel(\"../results/Control_Google_RR_merged.xlsx\")\n",
    "google_snip_control = pd.read_excel(\"../results/Control_Google_SERP_merged.xlsx\")\n",
    "gemini_control = pd.read_excel(\"../results/Control_gemini_merged.xlsx\")\n",
    "gpt_control = pd.read_excel(\"../results/Control_ChatGPT_merged.xlsx\")\n",
    "\n",
    "control_df = pd.concat([google_snip_control, google_RR_control, bing_snip_control, bing_RR_control, gemini_control, gpt_control], ignore_index=True)\n",
    "\n",
    "bing_RR_target = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip_target = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR_target = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip_target = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini_target = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt_target = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "\n",
    "target_df = pd.concat([google_snip_target, google_RR_target, bing_snip_target, bing_RR_target, gemini_target, gpt_target], ignore_index=True)\n",
    "\n",
    "control_df[\"user group\"] = [\"Control group\"]*len(control_df)\n",
    "target_df[\"user group\"] = [\"ASD group\"]*len(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, shapiro\n",
    "import pandas as pd\n",
    "\n",
    "def get_pval(resp_group, y):\n",
    "\n",
    "    # shapiro_1 = shapiro(control_df.loc[control_df[\"response_group\"]==resp_group][y])[1]\n",
    "    # shapiro_2 = shapiro(target_df.loc[target_df[\"response_group\"]==resp_group][y])[1]\n",
    "    # if shapiro_1 < 0.05 or shapiro_2 < 0.05:\n",
    "    #     test_name = \"Mann Whitney U test\"\n",
    "    #     test_stat, p_val = mannwhitneyu(control_df.loc[control_df[\"response_group\"]==resp_group][y], target_df.loc[target_df[\"response_group\"]==resp_group][y])\n",
    "    # else:\n",
    "    test_name = \"T-Test\"\n",
    "    test_stat, p_val = ttest_ind(control_df.loc[control_df[\"response_group\"]==resp_group][y], target_df.loc[target_df[\"response_group\"]==resp_group][y])\n",
    "\n",
    "    return test_name, test_stat, p_val\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "groups = [\n",
    "     'Google SERP', 'Bing SERP', 'Google RR', 'Bing RR', 'Gemini', 'GPT 3.5'\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for group_type in groups:\n",
    "     for marker in markers:\n",
    "        test_name, test_stat, p_val = get_pval(resp_group=group_type, y=marker)\n",
    "        results.append([group_type, marker, test_name, test_stat, p_val, bool(p_val < 0.05)])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Comparison', 'Marker', 'Significance test', 'Test stat', 'p-value', 'is significant'])\n",
    "results_df.to_excel(\"../stats/ControlvsTarget_GenComparison_SignificanceTests_new.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt], ignore_index=True)\n",
    "# merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR], ignore_index=True)\n",
    "merged_df[\"ngram_raw\"] = [len(row[\"query\"].split(\" \")) for _,row in merged_df.iterrows()]\n",
    "merged_df[\"ngram_raw\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         group    category_type   marker          test_name  test_stats  \\\n",
      "0  Google SERP            ngram  ParaLen  Linear Regression    0.415665   \n",
      "1  Google SERP  domain_specific  ParaLen             T-Test   -0.511024   \n",
      "2  Google SERP            ngram       SC  Linear Regression    0.395751   \n",
      "3  Google SERP  domain_specific       SC             T-Test   -0.966465   \n",
      "4  Google SERP            ngram  SentLen  Linear Regression   -0.081950   \n",
      "\n",
      "          p_val  is_significant  \n",
      "0  7.290057e-12               1  \n",
      "1  6.097891e-01               0  \n",
      "2  8.411862e-11               1  \n",
      "3  3.347529e-01               0  \n",
      "4  1.965569e-01               0  \n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [group, category_type, marker]\n",
      "Index: []\n",
      "\n",
      "132 55\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal, mannwhitneyu, linregress, ttest_ind \n",
    "import numpy as np\n",
    "\n",
    "markers = [\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"ratio_headings\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_paras\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "    ]\n",
    "\n",
    "marker_name = {\n",
    "        \"ratio_paras\": \"ParaSents\",\n",
    "        \"ratio_list_items\": \"ListSents\",\n",
    "        \"ratio_headings\": \"HeadingSents\",\n",
    "        \"avg_para_len\": \"ParaLen\",\n",
    "        \"num_sentences\": \"SC\",\n",
    "        \"avg_len\": \"SentLen\",\n",
    "        \"flesch\": \"FRES\",\n",
    "        \"cli\": \"CLI\",\n",
    "        \"avg_concrete\": \"Conc\",\n",
    "        \"concrete_ratio\": \"ConcreteWords\",\n",
    "        \"abstract_ratio\": \"AbstractWords\"\n",
    "}\n",
    "\n",
    "groups = [\n",
    "    \"Google SERP\",\n",
    "    \"Google RR\",\n",
    "    \"Bing SERP\",\n",
    "    \"Bing RR\",\n",
    "    \"Gemini\",\n",
    "    \"GPT 3.5\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "issues = []\n",
    "\n",
    "for group in groups:\n",
    "    group_df = merged_df.loc[merged_df[\"response_group\"]==group]\n",
    "    for marker in markers:\n",
    "        # ngram\n",
    "        try:\n",
    "            slope, intercept, r_val, p_val, se = linregress(group_df['ngram_raw'], group_df[marker])\n",
    "            test_name = \"Linear Regression\"\n",
    "            if r_val == None:\n",
    "                issues.append([group, \"ngram\", marker_name[marker]])\n",
    "            elif p_val < 0.05:\n",
    "                is_significant = 1\n",
    "            else:\n",
    "                is_significant = 0\n",
    "            results.append([group, \"ngram\", marker_name[marker], test_name, r_val, p_val, is_significant])\n",
    "        except:\n",
    "            issues.append([group, \"ngram\", marker_name[marker]])\n",
    "\n",
    "        # domain specific\n",
    "        domain_specific = group_df.loc[group_df[\"domain_specific\"]==True][marker]\n",
    "        domain_general = group_df.loc[group_df[\"domain_specific\"]==False][marker]\n",
    "        try:\n",
    "            t_val, p_val = ttest_ind(domain_specific, domain_general)\n",
    "            if p_val < 0.05:\n",
    "                is_significant = 1\n",
    "            else:\n",
    "                is_significant = 0\n",
    "            test_name = \"T-Test\"\n",
    "            results.append([group, \"domain_specific\", marker_name[marker], test_name, t_val, p_val, is_significant])\n",
    "        except:\n",
    "            issues.append([group, \"domain_specific\", marker_name[marker]])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns= [\"group\", \"category_type\", \"marker\", \"test_name\", \"test_stats\", \"p_val\", \"is_significant\"])\n",
    "issues_df = pd.DataFrame(issues, columns=[\"group\", \"category_type\", \"marker\"])\n",
    "print(results_df.head())\n",
    "print()\n",
    "print(issues_df)\n",
    "print()\n",
    "print(len(results_df), len(results_df.loc[results_df[\"p_val\"]<=0.01]))\n",
    "results_df.to_excel(\"../stats/QueryCategory_SignificanceTests_1.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, shapiro, ttest_ind\n",
    "\n",
    "def get_pval_pairwise(x, x_groups, y):\n",
    "    # shapiro_1 = shapiro(merged_df.loc[merged_df[x]==x_groups[0]][y])[1]\n",
    "    # shapiro_2 = shapiro(merged_df.loc[merged_df[x]==x_groups[1]][y])[1]\n",
    "    # if shapiro_1 < 0.05 or shapiro_2 < 0.05:\n",
    "    #     test_name = \"Mann Whitney U test\"\n",
    "    #     test_stat, p_val = mannwhitneyu(merged_df.loc[merged_df[x]==x_groups[0]][y], merged_df.loc[merged_df[x]==x_groups[1]][y])\n",
    "    # else:\n",
    "    test_name = \"T-Test\"\n",
    "    test_stat, p_val = ttest_ind(merged_df.loc[merged_df[x]==x_groups[0]][y], merged_df.loc[merged_df[x]==x_groups[1]][y])\n",
    "    return test_name, test_stat, p_val\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "groups_list = [\n",
    "    ['response_group_type', 'SERP', ['SERP', 'SERP Reformed']],\n",
    "    ['response_group_type', 'RR', ['RR', 'RR Reformed']],\n",
    "    ['response_group_type', 'Chatbot', ['Chatbot', 'Chatbot Reformed']],\n",
    "    ['response_group', 'Google SERP', ['Google SERP', 'Google SERP Reformed']],\n",
    "    ['response_group', 'Google RR',  ['Google RR', 'Google RR Reformed']],\n",
    "    ['response_group' , 'Bing SERP', ['Bing SERP', 'Bing SERP Reformed']],\n",
    "    ['response_group', 'Bing RR', ['Bing RR', 'Bing RR Reformed']],\n",
    "    ['response_group', 'Gemini', ['Gemini', 'Gemini Reformed']],\n",
    "    ['response_group', 'GPT 3.5', ['GPT 3.5', 'GPT 3.5 Reformed']]\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for group_type, group_name, groups in groups_list:\n",
    "    for marker in markers:\n",
    "        test_name, test_stat, p_val = get_pval_pairwise(x=group_type, x_groups=groups, y=marker)\n",
    "        results.append([group_name, marker, test_name, test_stat, p_val, bool(p_val < 0.05)])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Group name', 'marker', 'test name', 'test stat', 'p-val', 'is significant'])\n",
    "results_df.to_excel(\"../stats/QueryReformulation_SiginificanceTests.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group name</th>\n",
       "      <th>marker</th>\n",
       "      <th>test name</th>\n",
       "      <th>test stat</th>\n",
       "      <th>p-val</th>\n",
       "      <th>is significant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SERP</td>\n",
       "      <td>ratio_paras</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SERP</td>\n",
       "      <td>ratio_list_items</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERP</td>\n",
       "      <td>ratio_headings</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SERP</td>\n",
       "      <td>avg_para_len</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SERP</td>\n",
       "      <td>num_sentences</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>cli</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>avg_concrete</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>concrete_ratio</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>abstract_ratio</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>undefined_ratio</td>\n",
       "      <td>T-Test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Group name            marker test name  test stat  p-val  is significant\n",
       "0         SERP       ratio_paras    T-Test        NaN    NaN           False\n",
       "1         SERP  ratio_list_items    T-Test        NaN    NaN           False\n",
       "2         SERP    ratio_headings    T-Test        NaN    NaN           False\n",
       "3         SERP      avg_para_len    T-Test        NaN    NaN           False\n",
       "4         SERP     num_sentences    T-Test        NaN    NaN           False\n",
       "..         ...               ...       ...        ...    ...             ...\n",
       "103    GPT 3.5               cli    T-Test        NaN    NaN           False\n",
       "104    GPT 3.5      avg_concrete    T-Test        NaN    NaN           False\n",
       "105    GPT 3.5    concrete_ratio    T-Test        NaN    NaN           False\n",
       "106    GPT 3.5    abstract_ratio    T-Test        NaN    NaN           False\n",
       "107    GPT 3.5   undefined_ratio    T-Test        NaN    NaN           False\n",
       "\n",
       "[108 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOLandChildren",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
