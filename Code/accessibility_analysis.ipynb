{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from urllib3 import Retry\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_html(df, path):\n",
    "    html_content = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if idx % 50 == 0 and idx!=0:\n",
    "            time.sleep(10)\n",
    "        url = row[\"web_url\"]\n",
    "        try:\n",
    "            # html = urllib3.request(\"GET\", url, retries=Retry(5))\n",
    "            html = urllib3.request(\"GET\", url, retries=False)\n",
    "            if html.status == 200:\n",
    "                soup = BeautifulSoup(markup = html.data, features='lxml')\n",
    "                # kill all script and style elements\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.extract() # rip it out\n",
    "                html_content.append(str(soup.body))\n",
    "            else:\n",
    "                html_content.append(None)\n",
    "        except:\n",
    "            html_content.append(None)\n",
    "    df[\"html_content\"] = html_content\n",
    "    df.to_excel(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4ab4f047b54f1bb26ef5ee7fb906bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hrishita Chakrabarti\\AppData\\Local\\Temp\\ipykernel_2816\\1459555715.py:17: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(markup = html.data, features='lxml')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bc2d16da944242b466ee6539544b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "google = pd.read_excel(\"../data/Control_Google_results.xlsx\")\n",
    "scrape_html(df = google, path=\"../data/Control_Google_results.xlsx\")\n",
    "\n",
    "bing = pd.read_excel(\"../data/Control_Bing_results.xlsx\")\n",
    "scrape_html(df=bing, path=\"../data/Control_Bing_results.xlsx\")\n",
    "\n",
    "# google_reformed = pd.read_excel(\"../data/Google_QueryReformed_results.xlsx\")\n",
    "# scrape_html(df=google_reformed, path=\"../data/Google_QueryReformed_results.xlsx\")\n",
    "\n",
    "# bing_reformed = pd.read_excel(\"../data/Bing_QueryReformed_results.xlsx\")\n",
    "# scrape_html(df=bing_reformed, path=\"../data/Bing_QueryReformed_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessibility Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_text(content, url_flag):\n",
    "    if url_flag == 1:\n",
    "        \n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get text\n",
    "        try:\n",
    "            text = soup.body.get_text(separator='\\n ', strip=True)\n",
    "            num_list_items = len(soup.body.find_all(\"li\"))\n",
    "            num_headings = len(soup.body.find_all(\"h1\")) + len(soup.body.find_all(\"h2\")) + len(soup.body.find_all(\"h3\")) + len(soup.body.find_all(\"h4\")) + len(soup.body.find_all(\"h5\")) + len(soup.body.find_all(\"h6\"))\n",
    "            num_paras = 0\n",
    "            para_len = 0\n",
    "            num_inpara_headings = 0\n",
    "            for para in soup.body.find_all(\"p\"):\n",
    "                para_len += len(para.text.split())\n",
    "                num_paras += 1\n",
    "            avg_para_len = para_len/num_paras\n",
    "                    \n",
    "        except:\n",
    "            return None\n",
    "        return text, num_list_items, num_headings, num_inpara_headings, num_paras, avg_para_len\n",
    "    if url_flag == 0:\n",
    "        text = content\n",
    "        text = text.strip()\n",
    "        text = re.sub(\"\\n{2,}\", \"\\n\", text)\n",
    "        # print(text)\n",
    "        lines = text.split(\"\\n\")\n",
    "        # print(lines)\n",
    "        num_list_items = 0\n",
    "        num_headings = 0\n",
    "\n",
    "        for line in lines:\n",
    "            enum_match = re.search(\"\\d+\\.\\s\", line)\n",
    "            list_match = re.search(\"^\\*\\s\", line)\n",
    "            if enum_match != None or list_match != None:\n",
    "                num_list_items += 1\n",
    "\n",
    "        for line in lines:\n",
    "            heading_match = re.search(\"^\\*{2}(?:[^\\*]*)\\*{2}\", line) # **heading**\n",
    "            if heading_match != None:\n",
    "                # print(\"heading match \", line)\n",
    "                num_headings += 1\n",
    "\n",
    "        num_paras = 0\n",
    "        para_len = 0\n",
    "        num_inpara_headings = 0\n",
    "        for line in lines:\n",
    "            enum_match = re.search(\"\\d+\\.\\s\", line)\n",
    "            list_match = re.search(\"^\\*\\s\", line)\n",
    "            if enum_match == None and list_match == None:\n",
    "                heading_match = re.search(\"^\\*{2}(?:[^\\*]*)\\*{2}\", line)\n",
    "                if heading_match == None:\n",
    "                    para_len += len(line.split())\n",
    "                    num_paras += 1\n",
    "                else:\n",
    "                    line = re.sub(\"^\\*{2}(?:[^\\*]*)\\*{2}\", \"\", line)\n",
    "                    if line != \"\":\n",
    "                        para_len += len(line.split())\n",
    "                        num_inpara_headings += 1\n",
    "                        num_paras += 1\n",
    "        if num_paras != 0:\n",
    "            avg_para_len = para_len/num_paras\n",
    "        else:\n",
    "            avg_para_len = 0\n",
    "\n",
    "        return text, num_list_items, num_headings, num_inpara_headings, num_paras, avg_para_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hrishita Chakrabarti\\anaconda3\\envs\\WebAccessibility\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Hrishita Chakrabarti\\anaconda3\\envs\\WebAccessibility\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import syllables\n",
    "import contractions\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import textstat\n",
    "\n",
    "concrete_df = pd.read_excel(\"../Data/concreteness_data.xlsx\", index_col='Word') # 1 - maximally abstract, 5 - maximally concrete\n",
    "concrete_dict = concrete_df.to_dict('index')\n",
    "bigram_words = [word for word in concrete_dict.keys() if len(str(word).split())>1]\n",
    "# print(bigram_words)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def calc_concreteness(text):\n",
    "    sum_concrete = 0\n",
    "    concrete_count = 0\n",
    "    abstract_count = 0\n",
    "\n",
    "    for bigram in bigram_words:\n",
    "        if bigram in text:\n",
    "            num_occur = len([m.start() for m in re.finditer(bigram, text)])\n",
    "            text = re.sub(bigram, \"\", text)\n",
    "            concreteness = concrete_dict[bigram][\"Conc.M\"]\n",
    "            if round(concreteness) < 4:\n",
    "                abstract_count += num_occur\n",
    "            else:\n",
    "                concrete_count += num_occur\n",
    "            sum_concrete += (concreteness * num_occur)\n",
    "\n",
    "    for word in nlp(text):\n",
    "        if concrete_dict.get(word.text, None) != None:\n",
    "            concreteness = concrete_dict[word.text]['Conc.M']\n",
    "            if concreteness < 4:\n",
    "                abstract_count += 1\n",
    "            else:\n",
    "                concrete_count += 1\n",
    "            sum_concrete += concreteness\n",
    "    if concrete_count + abstract_count == 0:\n",
    "        avg_concreteness = 0\n",
    "    else:\n",
    "        avg_concreteness = round(sum_concrete/(abstract_count + concrete_count), 2)\n",
    "    concrete_ratio = round(concrete_count/len(nlp(text)), 2)\n",
    "    abstract_ratio = round(abstract_count/len(nlp(text)), 2)\n",
    "    return avg_concreteness, concrete_ratio, abstract_ratio  \n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    return round(textstat.flesch_reading_ease(text),2)\n",
    "    # num_words = 0\n",
    "    # num_sentences = 0\n",
    "    # num_syllables = 0\n",
    "\n",
    "    # text = text.lower()\n",
    "    # sentences = nltk.sent_tokenize(text)\n",
    "    # num_sentences = len(sentences)\n",
    "    # for sentence in sentences:\n",
    "    #     words = sentence.split(\" \")\n",
    "    #     num_words += len(words)\n",
    "    #     for word in words:\n",
    "    #         num_syllables += syllables.estimate(word)\n",
    "\n",
    "    # score = 206.8835 - (1.015*(num_words/num_sentences)) - (84.6 * (num_syllables/num_words))\n",
    "\n",
    "    # return round(score, 2)\n",
    "\n",
    "def coleman_liau(text):\n",
    "\n",
    "    return round(textstat.coleman_liau_index(text),2)\n",
    "\n",
    "    # num_letters = 0\n",
    "    # num_words = 0\n",
    "    # num_sentences = 0\n",
    "\n",
    "    # sentences = nltk.sent_tokenize(text)\n",
    "    # num_sentences = len(sentences)\n",
    "    # for sentence in sentences:\n",
    "    #     words = sentence.split(\" \")\n",
    "    #     num_words += len(words)\n",
    "    #     for word in words:\n",
    "    #         num_letters += len(word)\n",
    "\n",
    "    # L = (num_letters/num_words)*100\n",
    "    # S = (num_sentences/num_words)*100\n",
    "\n",
    "    # score = (0.0588 * L) - (0.296 * S) - 15.8\n",
    "\n",
    "    # return round(score, 2)\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = re.sub('\\\\n{2,}', '\\\\n', text)\n",
    "    text = re.sub(':', \"\\n\", text)\n",
    "    text = re.sub('\\\\t', ' ', text)\n",
    "    # text = re.sub('\\*+(?:\\S+)\\*+', '', text)\n",
    "    # text = re.sub('\\*', '', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub('[\\?\\!\\n]+', '. ', text)\n",
    "    text = re.sub('[^A-Za-z0-9\\s\\-\\_\\.]', '', text)\n",
    "    text = re.sub('\\.{2,}', '. ', text) # ... -> .\n",
    "    text = re.sub('\\s{2,}', ' ', text) # multiple spaces -> singular space\n",
    "    return text\n",
    "\n",
    "def text_analysis(text):\n",
    "    text = text_cleaning(text)\n",
    "    if text != \"\":\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        num_words = [len(sentence.split()) for sentence in sentences]\n",
    "        avg_len = round(sum(num_words)/len(sentences),2)\n",
    "        text = contractions.fix(text)\n",
    "        avg_concreteness, concrete_ratio, abstract_ratio = calc_concreteness(text)\n",
    "        flesch = flesch_reading_ease(text)\n",
    "        cli = coleman_liau(text)\n",
    "\n",
    "        return [\n",
    "            text,\n",
    "            len(sentences),\n",
    "            avg_len,\n",
    "            flesch,\n",
    "            cli,\n",
    "            avg_concreteness,\n",
    "            concrete_ratio,\n",
    "            abstract_ratio,\n",
    "        ]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_text(data[\"web_url\"].iloc[0], url_flag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def compute_markers(data, file_name, data_type):\n",
    "    default = [None, None, None, None, None, None, None]\n",
    "    ratio_list_items = None\n",
    "    ratio_headings = None\n",
    "    ratio_paras = None\n",
    "    avg_para_len = None\n",
    "\n",
    "    path = \"../results/\" + file_name + \".xlsx\"\n",
    "\n",
    "    if data_type == \"SE_RR\": # SE_RR\n",
    "        column_names = [\"query\", \"query_category\", \"web_title\", \"web_url\", \"cleaned_text\", \"ratio_paras\", \"ratio_list_items\", \"ratio_headings\", \"avg_para_len\", \"num_sentences\", \"avg_len\", \"flesch\", \"cli\", \"avg_concrete\", \"concrete_ratio\", \"abstract_ratio\"]\n",
    "    elif data_type == \"SE_SERP\": # SE_SERP\n",
    "        column_names = [\"query\", \"query_category\", \"web_title\", \"web_snippet\",  \"cleaned_text\", \"ratio_paras\", \"ratio_list_items\", \"ratio_headings\", \"avg_para_len\", \"num_sentences\", \"avg_len\", \"flesch\", \"cli\", \"avg_concrete\", \"concrete_ratio\", \"abstract_ratio\"]\n",
    "    elif data_type == \"GenAI\": # GenAI\n",
    "        column_names = [\"query\", \"query_category\", \"response_text\", \"cleaned_text\", \"ratio_paras\", \"ratio_list_items\", \"ratio_headings\", \"avg_para_len\", \"num_sentences\", \"avg_len\", \"flesch\", \"cli\", \"avg_concrete\", \"concrete_ratio\", \"abstract_ratio\"]\n",
    "    results = []\n",
    "\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        if data_type == \"SE_RR\":\n",
    "            text_results = get_text(row[\"html_content\"], url_flag = 1)\n",
    "        elif data_type == \"SE_SERP\":\n",
    "            web_title = row[\"web_title\"]\n",
    "            web_snippet = row[\"web_snippet\"]\n",
    "            text_results = get_text(\"\\n\".join([web_title, web_snippet]), url_flag = 0)\n",
    "        elif data_type == \"GenAI\":\n",
    "            text_results = get_text(row[\"response_text\"], url_flag = 0)\n",
    "        \n",
    "        if text_results != None:\n",
    "            text, num_list_items, num_headings, num_inpara_headings, num_paras, avg_para_len = text_results\n",
    "            if data_type == \"SE_SERP\":\n",
    "                num_headings += 1 # web title is counted as a heading for web snippets\n",
    "\n",
    "            scores = text_analysis(text)\n",
    "\n",
    "            if scores == None:\n",
    "                scores = default\n",
    "            else:\n",
    "                ratio_list_items = num_list_items/scores[1]\n",
    "                ratio_headings = num_headings/scores[1]\n",
    "                ratio_paras = 1 - (ratio_list_items + ratio_headings) + num_inpara_headings/scores[1]\n",
    "\n",
    "        else:\n",
    "            scores = default\n",
    "\n",
    "        if data_type == \"SE_RR\":\n",
    "            result = [row[\"query\"], row[\"query_category\"], row[\"web_title\"], row[\"web_url\"]] + [scores[0]] + [ratio_paras, ratio_list_items, ratio_headings, avg_para_len] + scores[1:]\n",
    "        elif data_type == \"SE_SERP\":\n",
    "            result = [row[\"query\"], row[\"query_category\"], row[\"web_title\"], row[\"web_snippet\"]] + [scores[0]] + [ratio_paras, ratio_list_items, ratio_headings, avg_para_len] + scores[1:]\n",
    "        elif data_type == \"GenAI\":\n",
    "            result = [row[\"query\"], row[\"query_category\"], row[\"response_text\"]] + [scores[0]] + [ratio_paras, ratio_list_items, ratio_headings, avg_para_len] + scores[1:]\n",
    "\n",
    "        results.append(result)\n",
    "        \n",
    "        if idx % 500 == 0 and idx!=0:\n",
    "            results_df = pd.DataFrame(results, columns = column_names)\n",
    "            results_df.to_excel(path, index=False)\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns = column_names)\n",
    "    results_df.to_excel(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "google = pd.read_excel(\"../Data/Control_Google_results.xlsx\").dropna()\n",
    "bing = pd.read_excel(\"../Data/Control_Bing_results.xlsx\").dropna()\n",
    "gemini = pd.read_excel(\"../Data/Control_Gemini_results.xlsx\").dropna()\n",
    "gpt = pd.read_excel(\"../Data/Control_ChatGPT_results.xlsx\").dropna()\n",
    "\n",
    "# google_reformed = pd.read_excel(\"../Data/Google_QueryReformed_results.xlsx\").dropna()\n",
    "# bing_reformed = pd.read_excel(\"../Data/Bing_QueryReformed_results.xlsx\").dropna()\n",
    "# gemini_reformed = pd.read_excel(\"../Data/Gemini_QueryReformed_results.xlsx\").dropna()\n",
    "# gpt_reformed = pd.read_excel(\"../Data/ChatGPT_QueryReformed_results.xlsx\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dab5d7d09b045738e5783c40b052add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4222120e3e814dbf9c79f8ef8f0eeeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a1bfab0bca42e48492bc4961651098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a982da5112f47d080bc3926f72198f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad58b6a851d4d1f86284ab5caf91319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd70a3d632ea4224b1df270ab15de9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_markers(data = google, file_name = \"Control_Google_RR_results\", data_type=\"SE_RR\")\n",
    "compute_markers(data = google, file_name = \"Control_Google_SERP_results\", data_type=\"SE_SERP\")\n",
    "compute_markers(data = bing, file_name = \"Control_Bing_RR_results\", data_type=\"SE_RR\")\n",
    "compute_markers(data = bing, file_name = \"Control_Bing_SERP_results\", data_type=\"SE_SERP\")\n",
    "compute_markers(data = gemini, file_name = \"Control_Gemini_results\", data_type=\"GenAI\")\n",
    "compute_markers(data = gpt, file_name = \"Control_ChatGPT_results\", data_type=\"GenAI\")\n",
    "\n",
    "# compute_markers(data = google_reformed, file_name = \"Google_QueryReformed_RR_results\", data_type=\"SE_RR\")\n",
    "# compute_markers(data = google_reformed, file_name = \"Google_QueryReformed_SERP_results\", data_type=\"SE_SERP\")\n",
    "# compute_markers(data = bing_reformed, file_name = \"Bing_QueryReformed_RR_results\", data_type=\"SE_RR\")\n",
    "# compute_markers(data = bing_reformed, file_name = \"Bing_QueryReformed_SERP_results\", data_type=\"SE_SERP\")\n",
    "# compute_markers(data = gemini_reformed, file_name = \"Gemini_QueryReformed_results\", data_type=\"GenAI\")\n",
    "# compute_markers(data = gpt_reformed, file_name = \"ChatGPT_QueryReformed_results\", data_type=\"GenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_results.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_results.xlsx\")\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_results.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_results.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/Gemini_results.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_results.xlsx\")\n",
    "\n",
    "google_RR_control = pd.read_excel(\"../results/Control_Google_RR_results.xlsx\")\n",
    "google_snip_control = pd.read_excel(\"../results/Control_Google_SERP_results.xlsx\")\n",
    "bing_RR_control = pd.read_excel(\"../results/Control_Bing_RR_results.xlsx\")\n",
    "bing_snip_control = pd.read_excel(\"../results/Control_Bing_SERP_results.xlsx\")\n",
    "gemini_control = pd.read_excel(\"../results/Control_Gemini_results.xlsx\")\n",
    "gpt_control = pd.read_excel(\"../results/Control_ChatGPT_results.xlsx\")\n",
    "\n",
    "google_reformed_RR = pd.read_excel(\"../results/Google_QueryReformed_RR_results.xlsx\")\n",
    "google_reformed_snip = pd.read_excel(\"../results/Google_QueryReformed_SERP_results.xlsx\")\n",
    "bing_reformed_RR = pd.read_excel(\"../results/Bing_QueryReformed_RR_results.xlsx\")\n",
    "bing_reformed_snip = pd.read_excel(\"../results/Bing_QueryReformed_SERP_results.xlsx\")\n",
    "gemini_reformed = pd.read_excel(\"../results/Gemini_QueryReformed_results.xlsx\")\n",
    "gpt_reformed = pd.read_excel(\"../results/ChatGPT_QueryReformed_results.xlsx\")\n",
    "\n",
    "# queries = pd.read_excel(\"../data/SelectedQueries_2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.dropna(how='any', inplace=False)\n",
    "    df = df.drop(df[df.num_sentences <=0].index, inplace=False)\n",
    "    df = df.drop(df[df.avg_len <=0].index, inplace=False)\n",
    "    # df = df.drop(df[df.flesch <=0].index, inplace=False)\n",
    "    # df = df.drop(df[df.cli <=0].index, inplace=False)\n",
    "    # df = df.drop(df[df.avg_concrete <=0].index, inplace=False)\n",
    "    # df = df.drop(df[df.concrete_ratio <=0].index, inplace=False)\n",
    "    # df = df.drop(df[df.abstract_ratio <=0].index, inplace=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_results(df, file_name, response_group_name, response_group_type, control=0):\n",
    "    if control == 0:\n",
    "        queries = pd.read_excel(\"../data/SelectedQueries_2.xlsx\")\n",
    "    else:\n",
    "        queries = pd.read_excel(\"../data/ControlQueries.xlsx\")\n",
    "    df = clean_df(df)\n",
    "    marker_columns = [\"ratio_paras\", \"ratio_list_items\", \"ratio_headings\", \"avg_para_len\", \"num_sentences\", \"avg_len\", \"flesch\", \"cli\", \"avg_concrete\", \"concrete_ratio\", \"abstract_ratio\"]\n",
    "    \n",
    "    df_merged = pd.DataFrame(\n",
    "        df.groupby([\"query\"]).mean(numeric_only=True).values.tolist(),\n",
    "        columns = marker_columns\n",
    "    )\n",
    "\n",
    "    df_merged = df_merged.round(2)\n",
    "    df_merged.insert(0, \"query\", df.groupby([\"query\"]).mean(numeric_only=True).index.tolist())\n",
    "    ngrams = []\n",
    "    domain_relevant = []\n",
    "    for query in df_merged[\"query\"]:\n",
    "        if len(query.split())>4:\n",
    "            ngrams.append(\">4\")\n",
    "        else:\n",
    "            ngrams.append(str(len(query.split())))\n",
    "        if control == 0:\n",
    "            domain_relevant.append(queries.loc[queries['query']==query]['domain_relevant'].tolist()[0])\n",
    "    df_merged.insert(1, \"ngram\", ngrams, True)\n",
    "    if control == 0:\n",
    "        df_merged.insert(2, \"domain_specific\", domain_relevant, True)\n",
    "    df_merged.insert(len(df_merged.columns), \"undefined_ratio\", 1-(df_merged[\"concrete_ratio\"] + df_merged[\"abstract_ratio\"]), True)\n",
    "\n",
    "    present_queries = df_merged[\"query\"].tolist()\n",
    "    all_queries = queries[\"query\"].tolist()\n",
    "\n",
    "    for query in all_queries:\n",
    "        if query not in present_queries:\n",
    "            marker_values = [0] * len(marker_columns)\n",
    "            ngram = len(query.split())\n",
    "            if ngram > 4:\n",
    "                ngram = \">4\"\n",
    "            else:\n",
    "                ngram = str(ngram)\n",
    "            if control == 0:\n",
    "                domain_relevant = queries.loc[queries['query']==query]['domain_relevant'].tolist()[0]   \n",
    "            undefined_ratio = 0\n",
    "            if control == 0:\n",
    "                row = [query, ngram, domain_relevant] + marker_values + [undefined_ratio]\n",
    "            else:\n",
    "                row = [query, ngram] + marker_values + [undefined_ratio]\n",
    "            df_merged.loc[len(df_merged), df_merged.columns] = row     \n",
    "\n",
    "    df_merged['response_group'] = [response_group_name] * len(df_merged)\n",
    "    df_merged['response_group_type'] = [response_group_type] * len(df_merged)\n",
    "\n",
    "    path = '../results/'+file_name\n",
    "    df_merged.to_excel(path, index=False)\n",
    "\n",
    "merge_results(google_RR, \"Google_RR_merged.xlsx\", response_group_name='Google RR', response_group_type='RR')\n",
    "merge_results(google_snip, \"Google_SERP_merged.xlsx\", response_group_name='Google SERP', response_group_type='SERP')\n",
    "merge_results(bing_RR, \"Bing_RR_merged.xlsx\", response_group_name='Bing RR', response_group_type='RR')\n",
    "merge_results(bing_snip, \"Bing_SERP_merged.xlsx\", response_group_name='Bing SERP', response_group_type='SERP')\n",
    "merge_results(gemini, \"Gemini_merged.xlsx\", response_group_name='Gemini', response_group_type='Chatbot')\n",
    "merge_results(gpt, \"ChatGPT_merged.xlsx\", response_group_name='GPT 3.5', response_group_type='Chatbot')\n",
    "\n",
    "merge_results(google_RR_control, \"Control_Google_RR_merged.xlsx\", response_group_name='Google RR', response_group_type='RR', control=1)\n",
    "merge_results(google_snip_control, \"Control_Google_SERP_merged.xlsx\", response_group_name='Google SERP', response_group_type='SERP', control=1)\n",
    "merge_results(bing_RR_control, \"Control_Bing_RR_merged.xlsx\", response_group_name='Bing RR', response_group_type='RR', control=1)\n",
    "merge_results(bing_snip_control, \"Control_Bing_SERP_merged.xlsx\", response_group_name='Bing SERP', response_group_type='SERP', control=1)\n",
    "merge_results(gemini_control, \"Control_Gemini_merged.xlsx\", response_group_name='Gemini', response_group_type='Chatbot', control=1)\n",
    "merge_results(gpt_control, \"Control_ChatGPT_merged.xlsx\", response_group_name='GPT 3.5', response_group_type='Chatbot', control=1)\n",
    "\n",
    "merge_results(google_reformed_RR, \"Google_QueryReformed_RR_merged.xlsx\", response_group_name='Google RR Reformed', response_group_type='RR Reformed')\n",
    "merge_results(google_reformed_snip, \"Google_QueryReformed_SERP_merged.xlsx\", response_group_name='Google SERP Reformed', response_group_type='SERP Reformed')\n",
    "merge_results(bing_reformed_RR, \"Bing_QueryReformed_RR_merged.xlsx\", response_group_name='Bing RR Reformed', response_group_type='RR Reformed')\n",
    "merge_results(bing_reformed_snip, \"Bing_QueryReformed_SERP_merged.xlsx\", response_group_name='Bing SERP Reformed', response_group_type='SERP Reformed')\n",
    "merge_results(gemini_reformed, \"Gemini_QueryReformed_merged.xlsx\", response_group_name='Gemini Reformed', response_group_type='Chatbot Reformed')\n",
    "merge_results(gpt_reformed, \"ChatGPT_QueryReformed_merged.xlsx\", response_group_name='GPT 3.5 Reformed', response_group_type='Chatbot Reformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_comparison_desc_stats(merged_df, file_name):\n",
    "    markers = [\n",
    "            \"ratio_paras\",\n",
    "            \"ratio_list_items\",\n",
    "            \"ratio_headings\",\n",
    "            \"avg_para_len\",\n",
    "            \"num_sentences\",\n",
    "            \"avg_len\",\n",
    "            \"flesch\",\n",
    "            \"cli\",\n",
    "            \"avg_concrete\",\n",
    "            \"concrete_ratio\",\n",
    "            \"abstract_ratio\",\n",
    "            \"undefined_ratio\"\n",
    "        ]\n",
    "\n",
    "    response_group_types = [\n",
    "        \"SERP\",\n",
    "        \"RR\",\n",
    "        \"Chatbot\"\n",
    "    ]\n",
    "\n",
    "    response_groups = [\n",
    "        \"Google SERP\",\n",
    "        \"Google RR\",\n",
    "        \"Bing SERP\",\n",
    "        \"Bing RR\",\n",
    "        \"Gemini\",\n",
    "        \"GPT 3.5\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for response_group_type in response_group_types:\n",
    "        df = merged_df.loc[merged_df[\"response_group_type\"]==response_group_type]\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            results.append([response_group_type, marker, mean, std, median])\n",
    "\n",
    "    for response_group in response_groups:\n",
    "        df = merged_df.loc[merged_df[\"response_group\"]==response_group]\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            results.append([response_group, marker, mean, std, median])\n",
    "\n",
    "\n",
    "    desc_stats_gen_comparison = pd.DataFrame(results, columns=[\"group_name\", \"marker\", \"mean\", \"std\", \"median\"])\n",
    "    desc_stats_gen_comparison.to_excel(\"../results/\" + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "\n",
    "target_merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt], ignore_index=True)\n",
    "\n",
    "\n",
    "bing_RR_control = pd.read_excel(\"../results/Control_Bing_RR_merged.xlsx\")\n",
    "bing_snip_control = pd.read_excel(\"../results/Control_Bing_SERP_merged.xlsx\")\n",
    "google_RR_control = pd.read_excel(\"../results/Control_Google_RR_merged.xlsx\")\n",
    "google_snip_control = pd.read_excel(\"../results/Control_Google_SERP_merged.xlsx\")\n",
    "gemini_control = pd.read_excel(\"../results/Control_gemini_merged.xlsx\")\n",
    "gpt_control = pd.read_excel(\"../results/Control_ChatGPT_merged.xlsx\")\n",
    "control_merged_df = pd.concat([google_snip_control, google_RR_control, bing_snip_control, bing_RR_control, gemini_control, gpt_control], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_comparison_desc_stats(target_merged_df, \"DescriptiveStatsGenComparison.xlsx\")\n",
    "gen_comparison_desc_stats(control_merged_df, \"Control_DescriptiveStatsGenComparison.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = target_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "response_groups = [\n",
    "    \"Google SERP\",\n",
    "    \"Google RR\",\n",
    "    \"Bing SERP\",\n",
    "    \"Bing RR\",\n",
    "    \"Gemini\",\n",
    "    \"GPT 3.5\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for response_group in response_groups:\n",
    "    for ngram in [\"1\", \"2\", \"3\", \"4\", \">4\"]:\n",
    "        df = merged_df.loc[(merged_df[\"response_group\"]==response_group) & (merged_df['ngram']==ngram)]\n",
    "        if df.empty:\n",
    "            print(ngram, response_group)\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            results.append([response_group, str(ngram) + \"-gram\", marker, mean, std, median])\n",
    "    \n",
    "    for domain in [True, False]:\n",
    "        df = merged_df.loc[(merged_df[\"response_group\"]==response_group) & (merged_df['domain_specific']==domain)]\n",
    "        if df.empty:\n",
    "            print(domain, response_group)\n",
    "        for marker in markers:\n",
    "            mean = round(np.mean(df[marker].to_numpy()), 2)\n",
    "            std = round(np.std(df[marker].to_numpy()), 2)\n",
    "            median = round(np.median(df[marker].to_numpy()), 2)\n",
    "            if domain == True:\n",
    "                category = \"domain specific\"\n",
    "            else:\n",
    "                category = \"general\"\n",
    "            results.append([response_group, category, marker, mean, std, median])\n",
    "\n",
    "\n",
    "desc_stats_query_category = pd.DataFrame(results, columns=[\"group_name\", \"query_category\", \"marker\", \"mean\", \"std\", \"median\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats_query_category.to_excel(\"../results/DescriptiveStatsQueryCategory.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bing_RR = pd.read_excel(\"../results/Bing_RR_merged.xlsx\")\n",
    "bing_snip = pd.read_excel(\"../results/Bing_SERP_merged.xlsx\")\n",
    "google_RR = pd.read_excel(\"../results/Google_RR_merged.xlsx\")\n",
    "google_snip = pd.read_excel(\"../results/Google_SERP_merged.xlsx\")\n",
    "gemini = pd.read_excel(\"../results/gemini_merged.xlsx\")\n",
    "gpt = pd.read_excel(\"../results/ChatGPT_merged.xlsx\")\n",
    "google_reformed_RR = pd.read_excel(\"../results/Google_QueryReformed_RR_merged.xlsx\")\n",
    "google_reformed_snip = pd.read_excel(\"../results/Google_QueryReformed_SERP_merged.xlsx\")\n",
    "bing_reformed_RR = pd.read_excel(\"../results/Bing_QueryReformed_RR_merged.xlsx\")\n",
    "bing_reformed_snip = pd.read_excel(\"../results/Bing_QueryReformed_SERP_merged.xlsx\")\n",
    "gemini_reformed = pd.read_excel(\"../results/Gemini_QueryReformed_merged.xlsx\")\n",
    "gpt_reformed = pd.read_excel(\"../results/ChatGPT_QueryReformed_merged.xlsx\")\n",
    "\n",
    "merged_df = pd.concat([google_snip, google_RR, bing_snip, bing_RR, gemini, gpt, google_reformed_snip, google_reformed_RR, bing_reformed_snip, bing_reformed_RR, gemini_reformed, gpt_reformed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "markers = [\n",
    "        \"ratio_paras\",\n",
    "        \"ratio_list_items\",\n",
    "        \"ratio_headings\",\n",
    "        \"avg_para_len\",\n",
    "        \"num_sentences\",\n",
    "        \"avg_len\",\n",
    "        \"flesch\",\n",
    "        \"cli\",\n",
    "        \"avg_concrete\",\n",
    "        \"concrete_ratio\",\n",
    "        \"abstract_ratio\",\n",
    "        \"undefined_ratio\"\n",
    "    ]\n",
    "\n",
    "response_group_types = [\n",
    "    [\"SERP\", \"SERP Reformed\"],\n",
    "    [\"RR\", \"RR Reformed\"],\n",
    "    [\"Chatbot\", \"Chatbot Reformed\"],\n",
    "]\n",
    "\n",
    "response_groups = [\n",
    "    [\"Google SERP\", \"Google SERP Reformed\"],\n",
    "    [\"Google RR\", \"Google RR Reformed\"],\n",
    "    [\"Bing SERP\", \"Bing SERP Reformed\"],\n",
    "    [\"Bing RR\", \"Bing RR Reformed\"],\n",
    "    [\"Gemini\", \"Gemini Reformed\"],\n",
    "    [\"GPT 3.5\", \"GPT 3.5 Reformed\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for response_group_type in response_group_types:\n",
    "    original = response_group_type[0]\n",
    "    reformed = response_group_type[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group_type\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group_type\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_diff = round(np.mean(df_reformed[marker].to_numpy()) - np.mean(df_original[marker].to_numpy()), 2)\n",
    "        std_diff = round(np.std(df_reformed[marker].to_numpy()) - np.std(df_original[marker].to_numpy()), 2)\n",
    "        median_diff = round(np.median(df_reformed[marker].to_numpy()) - np.median(df_original[marker].to_numpy()), 2)\n",
    "        results.append([response_group_type[0], marker, mean_diff, std_diff, median_diff])\n",
    "\n",
    "for response_group in response_groups:\n",
    "    original = response_group[0]\n",
    "    reformed = response_group[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_original = np.mean(df_original[marker].to_numpy())\n",
    "        mean_reformed = np.mean(df_reformed[marker].to_numpy()) \n",
    "        std_original = np.std(df_original[marker].to_numpy())\n",
    "        std_reformed = np.std(df_reformed[marker].to_numpy())\n",
    "        median_original = np.median(df_original[marker].to_numpy())\n",
    "        median_reformed = np.median(df_reformed[marker].to_numpy())\n",
    "        results.append([response_group[0], marker, mean_original, std_original, median_original, mean_reformed, std_reformed, median_reformed])\n",
    "\n",
    "\n",
    "desc_stats_query_reformulation = pd.DataFrame(results, columns=[\"group_name\", \"marker\", \"mean_original\", \"std_original\", \"median_original\", \"mean_reformed\", \"std_reformed\", \"median_reformed\"])\n",
    "desc_stats_query_reformulation.to_excel(\"../results/DescriptiveStatsQueryReformulation.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for response_group_type in response_group_types:\n",
    "    original = response_group_type[0]\n",
    "    reformed = response_group_type[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group_type\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group_type\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_diff = round(np.mean(df_reformed[marker].to_numpy()) - np.mean(df_original[marker].to_numpy()), 2)\n",
    "        std_diff = round(np.std(df_reformed[marker].to_numpy()) - np.std(df_original[marker].to_numpy()), 2)\n",
    "        median_diff = round(np.median(df_reformed[marker].to_numpy()) - np.median(df_original[marker].to_numpy()), 2)\n",
    "        results.append([response_group_type[0], marker, mean_diff, std_diff, median_diff])\n",
    "\n",
    "for response_group in response_groups:\n",
    "    original = response_group[0]\n",
    "    reformed = response_group[1]\n",
    "    df_original = merged_df.loc[merged_df[\"response_group\"] == original]\n",
    "    df_reformed = merged_df.loc[merged_df[\"response_group\"] == reformed]\n",
    "    for marker in markers:\n",
    "        mean_diff = round(np.mean(df_reformed[marker].to_numpy()) - np.mean(df_original[marker].to_numpy()), 2)\n",
    "        std_diff = round(np.std(df_reformed[marker].to_numpy()) - np.std(df_original[marker].to_numpy()), 2)\n",
    "        median_diff = round(np.median(df_reformed[marker].to_numpy()) - np.median(df_original[marker].to_numpy()), 2)\n",
    "        results.append([response_group[0], marker, mean_diff, std_diff, median_diff])\n",
    "\n",
    "\n",
    "desc_stats_query_reformulation = pd.DataFrame(results, columns=[\"group_name\", \"marker\", \"mean_diff\", \"std_diff\", \"median_diff\"])\n",
    "desc_stats_query_reformulation.to_excel(\"../results/DescriptiveStatsQueryReformulation_Difference.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebAccessibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
