{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing query files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "target_og_query_file_path = \"\"\n",
    "target_rephrased_query_file_path = \"\"\n",
    "control_query_file_path = \"\"\n",
    "\n",
    "target_og_queries_df = pd.read_excel(target_og_query_file_path)\n",
    "target_rephrased_queries_df = pd.read_excel(target_rephrased_query_file_path)\n",
    "control_queries_df = pd.read_excel(control_query_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b75e4965d8c4496b7ada75ad4eb5b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import httpx\n",
    "from urllib.request import Request, urlopen\n",
    "from inscriptis import get_text\n",
    "from datetime import datetime\n",
    "\n",
    "f = open(\"API_keys.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "key = data[\"bing\"][\"key\"]\n",
    "SERP_endpoint = data[\"bing\"][\"SERP_endpoint\"]\n",
    "location = data[\"bing\"][\"location\"]\n",
    "\n",
    "f.close()\n",
    "\n",
    "# headers = {\n",
    "#             'Ocp-Apim-Subscription-Key': key1,\n",
    "            \n",
    "#         }\n",
    "\n",
    "headers = {\n",
    "            'Ocp-Apim-Subscription-Key': key,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'\n",
    "        }\n",
    "\n",
    "def get_bing_results(queries_df, result_file_name):\n",
    "    SERP_results = []\n",
    "    today = datetime.now()\n",
    "    today = today.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "    # queries_df = queries_df.iloc[range(0,5)]\n",
    "\n",
    "    for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "        query = row[\"query\"] \n",
    "        query_category = str(row[\"ngram\"]) + '-gram'\n",
    "\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'answerCount': 10, # number of results to be displayed\n",
    "            'setLang': 'en',\n",
    "            'mkt': 'en-US' # english in US (local market resulting in results in Dutch)\n",
    "        }\n",
    "\n",
    "        SERP_response = httpx.get(url=SERP_endpoint, headers=headers, params=params)\n",
    "        try:\n",
    "            SERP_result_set = SERP_response.json()\n",
    "            asked_query = SERP_result_set['queryContext']['originalQuery']\n",
    "            for result in SERP_result_set['webPages']['value']:\n",
    "                web_title =  result[\"name\"]\n",
    "                web_url = result[\"url\"]\n",
    "                web_snippet = result[\"snippet\"]\n",
    "                # try:\n",
    "                #     req = Request(\n",
    "                #         url= web_url, \n",
    "                #         headers={'User-Agent': 'Mozilla/5.0'}\n",
    "                #     )\n",
    "                #     html = urlopen(req).read().decode('utf-8')\n",
    "                #     # html = urllib.request.urlopen(web_url).read().decode('utf-8')\n",
    "                #     # web_text = get_text(html) \n",
    "                # except Exception as error:\n",
    "                #     # print(error)\n",
    "                #     # print(web_url)\n",
    "                #     web_text = None\n",
    "                \n",
    "                # SERP_results.append([query, query_category, web_title, web_url, web_snippet, web_text, today])\n",
    "                SERP_results.append([query, asked_query, query_category, web_title, web_url, web_snippet, today])\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            # print(reformed_query)\n",
    "            SERP_results.append([query, asked_query, query_category, None, None, None, today])\n",
    "\n",
    "    # SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"query_category\", \"web_title\", \"web_url\", \"web_snippet\", \"web_text\", \"date_crawled\"])\n",
    "    SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"asked_query\", \"query_category\", \"web_title\", \"web_url\", \"web_snippet\", \"date_crawled\"])\n",
    "    SERP_df.to_excel(\"../data/\" + result_file_name + \".xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bing_results(queries_df=target_og_queries_df, result_file_name=\"Bing_results\")\n",
    "get_bing_results(queries_df=target_rephrased_queries_df, result_file_name=\"Bing_QueryReformed_results\")\n",
    "get_bing_results(queries_df=control_queries_df, result_file_name=\"Control_Bing_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3df8eee86143aeb628a62bf8d81bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSDAWBB.NET\n",
      "\"www.xltits.net\"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "f = open(\"API_keys.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "API_KEY = data[\"google\"][\"api_key\"]\n",
    "SEARCH_ENGINE_ID = data[\"google\"][\"search_engine_id\"]\n",
    "SERP_endpoint = data[\"google\"][\"SERP_endpoint\"]\n",
    "f.close()\n",
    "\n",
    "# payload = {\n",
    "#         'key': API_KEY,\n",
    "#         'q': query,\n",
    "#         'cx': SEARCH_ENGINE_ID\n",
    "#     }\n",
    "\n",
    "# SERP_response = requests.get(url=SERP_endpoint, params=payload)\n",
    "# SERP_result_set = SERP_response.json()\n",
    "# SERP_result_set[\"items\"]\n",
    "\n",
    "def get_google_results(queries_df, results_file_name):\n",
    "\n",
    "    SERP_results = []\n",
    "    today = datetime.now()\n",
    "    today = today.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "    for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "        query = row[\"query\"] \n",
    "        query_category = str(row[\"ngram\"]) + '-gram'\n",
    "\n",
    "        payload = {\n",
    "            'key': API_KEY,\n",
    "            'q': query,\n",
    "            'cx': SEARCH_ENGINE_ID,\n",
    "            'lr': \"lang_en\"\n",
    "        }\n",
    "\n",
    "        SERP_response = requests.get(url=SERP_endpoint, params=payload)\n",
    "        try:\n",
    "            SERP_result_set = SERP_response.json()\n",
    "            asked_query = payload['q']\n",
    "            for result in SERP_result_set[\"items\"]:\n",
    "                try:\n",
    "                    web_title =  result[\"title\"]\n",
    "                except:\n",
    "                    web_title = None\n",
    "                try:\n",
    "                    web_url = result[\"link\"]\n",
    "                except:\n",
    "                    web_url = None\n",
    "                try:\n",
    "                    web_snippet = result[\"snippet\"]\n",
    "                except:\n",
    "                    web_snippet = None\n",
    "\n",
    "                # try:\n",
    "                #     req = Request(\n",
    "                #         url= web_url, \n",
    "                #         headers={'User-Agent': 'Mozilla/5.0'}\n",
    "                #     )\n",
    "                #     html = urlopen(req).read().decode('utf-8')\n",
    "                #     # html = urllib.request.urlopen(web_url).read().decode('utf-8')\n",
    "                #     # web_text = get_text(html) \n",
    "                # except Exception as error:\n",
    "                #     # print(error)\n",
    "                #     # print(web_url)\n",
    "                #     web_text = None\n",
    "                \n",
    "                SERP_results.append([query, asked_query, query_category, web_title, web_url, web_snippet, today])\n",
    "        except:\n",
    "            print(query)\n",
    "            SERP_results.append([query, asked_query, query_category, None, None, None, today])\n",
    "\n",
    "    SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"asked_query\", \"query_category\", \"web_title\", \"web_url\", \"web_snippet\", \"date_crawled\"])\n",
    "    SERP_df.to_excel(\"../data/\" + results_file_name + \".xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_google_results(queries_df=target_og_queries_df, result_file_name=\"Google_results\")\n",
    "get_google_results(queries_df=target_rephrased_queries_df, result_file_name=\"Google_QueryReformed_results\")\n",
    "get_google_results(queries_df=control_queries_df, result_file_name=\"Control_Google_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from datetime import datetime \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = GenerativeModel(\"gemini-pro-vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a88729cd704c7dace62bd1e1f35009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No repsonse generated for  2  queries.\n"
     ]
    }
   ],
   "source": [
    "def get_gemini_results(queries_df, results_file_name):\n",
    "\n",
    "    SERP_results = []\n",
    "    # queries_df = queries_df.iloc[range(0,5)]\n",
    "    today = datetime.now()\n",
    "    today = today.strftime(\"%Y_%m_%d\")\n",
    "    failed_query = 0\n",
    "\n",
    "    for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "        query = row[\"query\"]\n",
    "        query_category = str(row[\"ngram\"]) + '-gram'\n",
    "\n",
    "        response = model.generate_content(\n",
    "            [query]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response_text = response.candidates[0].content.parts[0].text\n",
    "        except:\n",
    "            failed_query += 1\n",
    "\n",
    "        SERP_results.append([query, query_category, response_text, today])\n",
    "\n",
    "    SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"query_category\", \"response_text\", \"date_crawled\"])\n",
    "    SERP_df.to_excel(\"../data/\" + results_file_name + \".xlsx\", index=False)\n",
    "    print(\"No repsonse generated for \", failed_query, \" queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gemini_results(queries_df=target_og_queries_df, result_file_name=\"Gemini_results\")\n",
    "get_gemini_results(queries_df=target_rephrased_queries_df, result_file_name=\"Gemini_QueryReformed_results\")\n",
    "get_gemini_results(queries_df=control_queries_df, result_file_name=\"Control_Gemini_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc33a9748774f009adc557b079e24ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No repsonse generated for  0  queries.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from datetime import datetime \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "f = open(\"API_keys.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "API_KEY = data[\"chatgpt\"][\"api_key\"]\n",
    "f.close()\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "model_id = 'gpt-3.5-turbo'\n",
    "\n",
    "def get_gpt_results(queries_df, results_file_name):\n",
    "\n",
    "    SERP_results = []\n",
    "    # queries_df = queries_df.iloc[range(0,5)]\n",
    "    today = datetime.now()\n",
    "    today = today.strftime(\"%Y_%m_%d\")\n",
    "    failed_query = 0\n",
    "\n",
    "    for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "        query = row[\"query\"]\n",
    "        # reformed_query = \"I am autistic please explain \" +  query + \" to me\"\n",
    "        query_category = str(row[\"ngram\"]) + '-gram'\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "            model=model_id,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response_text = response.choices[0].message.content\n",
    "        except:\n",
    "            failed_query += 1\n",
    "        SERP_results.append([query, query_category, response_text, today])\n",
    "\n",
    "    SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"query_category\", \"response_text\", \"date_crawled\"])\n",
    "    SERP_df.to_excel(\"../data/\" + results_file_name + \".xlsx\", index=False)\n",
    "    print(\"No repsonse generated for \", failed_query, \" queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt_results(queries_df=target_og_queries_df, result_file_name=\"ChatGPT_results\")\n",
    "get_gpt_results(queries_df=target_rephrased_queries_df, result_file_name=\"ChatGPT_QueryReformed_results\")\n",
    "get_gpt_results(queries_df=control_queries_df, result_file_name=\"Control_ChatGPT_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebAccessibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
