{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# sample_queries_df = pd.read_csv(\"../Data/searchData.csv\")\n",
    "# sample_queries = sample_queries_df[\"TaskDescription1\"].tolist()\n",
    "# query_category = [\"sample\"]*len(sample_queries)\n",
    "# sample_df = pd.DataFrame([], columns=['query', 'category'])\n",
    "# sample_df['query'] = sample_queries\n",
    "# sample_df['category'] = query_category\n",
    "# sample_df.to_csv(\"../Data/sampleQueries.csv\", index=False)\n",
    "# sample_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# queries_df = pd.read_excel(\"../Data/SelectedQueries_2.xlsx\")\n",
    "queries_df = pd.read_excel(\"../Data/ControlQueries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b75e4965d8c4496b7ada75ad4eb5b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import httpx\n",
    "from urllib.request import Request, urlopen\n",
    "from inscriptis import get_text\n",
    "from datetime import datetime\n",
    "\n",
    "f = open(\"API_keys.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "key1 = data[\"bing\"][\"key1\"]\n",
    "key2 = data[\"bing\"][\"key2\"]\n",
    "SERP_endpoint = data[\"bing\"][\"SERP_endpoint\"]\n",
    "location = data[\"bing\"][\"location\"]\n",
    "\n",
    "f.close()\n",
    "\n",
    "# headers = {\n",
    "#             'Ocp-Apim-Subscription-Key': key1,\n",
    "            \n",
    "#         }\n",
    "\n",
    "headers = {\n",
    "            'Ocp-Apim-Subscription-Key': key1,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'\n",
    "        }\n",
    "\n",
    "SERP_results = []\n",
    "today = datetime.now()\n",
    "today = today.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "# queries_df = queries_df.iloc[range(0,5)]\n",
    "\n",
    "for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "    query = row[\"query\"] \n",
    "    # reformed_query = query + \" I'm autistic\"\n",
    "    query_category = str(row[\"ngram\"]) + '-gram'\n",
    "\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'answerCount': 10, # number of results to be displayed\n",
    "        # 'count': 50, # max number of responses to be collected\n",
    "        # \"offset\": 0, # if results > count, how many results to be skipped from the beginning\n",
    "        'setLang': 'en',\n",
    "        # 'freshness': 'Month', # return webpages discovered within last 30 days\n",
    "        'mkt': 'en-US' # english in US (local market resulting in results in Dutch)\n",
    "    }\n",
    "\n",
    "    SERP_response = httpx.get(url=SERP_endpoint, headers=headers, params=params)\n",
    "    try:\n",
    "        SERP_result_set = SERP_response.json()\n",
    "        asked_query = SERP_result_set['queryContext']['originalQuery']\n",
    "        for result in SERP_result_set['webPages']['value']:\n",
    "            web_title =  result[\"name\"]\n",
    "            web_url = result[\"url\"]\n",
    "            web_snippet = result[\"snippet\"]\n",
    "            # try:\n",
    "            #     req = Request(\n",
    "            #         url= web_url, \n",
    "            #         headers={'User-Agent': 'Mozilla/5.0'}\n",
    "            #     )\n",
    "            #     html = urlopen(req).read().decode('utf-8')\n",
    "            #     # html = urllib.request.urlopen(web_url).read().decode('utf-8')\n",
    "            #     # web_text = get_text(html) \n",
    "            # except Exception as error:\n",
    "            #     # print(error)\n",
    "            #     # print(web_url)\n",
    "            #     web_text = None\n",
    "            \n",
    "            # SERP_results.append([query, query_category, web_title, web_url, web_snippet, web_text, today])\n",
    "            SERP_results.append([query, asked_query, query_category, web_title, web_url, web_snippet, today])\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        # print(reformed_query)\n",
    "        SERP_results.append([query, asked_query, query_category, None, None, None, today])\n",
    "\n",
    "# SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"query_category\", \"web_title\", \"web_url\", \"web_snippet\", \"web_text\", \"date_crawled\"])\n",
    "SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"asked_query\", \"query_category\", \"web_title\", \"web_url\", \"web_snippet\", \"date_crawled\"])\n",
    "SERP_df.to_excel(\"../data/Control_Bing_results.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# queries_df = pd.read_excel(\"../Data/SelectedQueries_2.xlsx\")\n",
    "queries_df = pd.read_excel(\"../Data/ControlQueries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3df8eee86143aeb628a62bf8d81bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSDAWBB.NET\n",
      "\"www.xltits.net\"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "f = open(\"API_keys.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "API_KEY = data[\"google\"][\"api_key\"]\n",
    "SEARCH_ENGINE_ID = data[\"google\"][\"search_engine_id\"]\n",
    "SERP_endpoint = data[\"google\"][\"SERP_endpoint\"]\n",
    "f.close()\n",
    "\n",
    "# payload = {\n",
    "#         'key': API_KEY,\n",
    "#         'q': query,\n",
    "#         'cx': SEARCH_ENGINE_ID\n",
    "#     }\n",
    "\n",
    "# SERP_response = requests.get(url=SERP_endpoint, params=payload)\n",
    "# SERP_result_set = SERP_response.json()\n",
    "# SERP_result_set[\"items\"]\n",
    "\n",
    "SERP_results = []\n",
    "# queries_df = queries_df.iloc[range(0,5)]\n",
    "today = datetime.now()\n",
    "today = today.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "    query = row[\"query\"] \n",
    "    # reformed_query = query + \" I'm autistic\"\n",
    "    query_category = str(row[\"ngram\"]) + '-gram'\n",
    "\n",
    "    payload = {\n",
    "        'key': API_KEY,\n",
    "        'q': query,\n",
    "        # 'q': reformed_query,\n",
    "        'cx': SEARCH_ENGINE_ID,\n",
    "        'lr': \"lang_en\"\n",
    "    }\n",
    "\n",
    "    SERP_response = requests.get(url=SERP_endpoint, params=payload)\n",
    "    try:\n",
    "        SERP_result_set = SERP_response.json()\n",
    "        asked_query = payload['q']\n",
    "        for result in SERP_result_set[\"items\"]:\n",
    "            try:\n",
    "                web_title =  result[\"title\"]\n",
    "            except:\n",
    "                web_title = None\n",
    "            try:\n",
    "                web_url = result[\"link\"]\n",
    "            except:\n",
    "                web_url = None\n",
    "            try:\n",
    "                web_snippet = result[\"snippet\"]\n",
    "            except:\n",
    "                web_snippet = None\n",
    "\n",
    "            # try:\n",
    "            #     req = Request(\n",
    "            #         url= web_url, \n",
    "            #         headers={'User-Agent': 'Mozilla/5.0'}\n",
    "            #     )\n",
    "            #     html = urlopen(req).read().decode('utf-8')\n",
    "            #     # html = urllib.request.urlopen(web_url).read().decode('utf-8')\n",
    "            #     # web_text = get_text(html) \n",
    "            # except Exception as error:\n",
    "            #     # print(error)\n",
    "            #     # print(web_url)\n",
    "            #     web_text = None\n",
    "            \n",
    "            SERP_results.append([query, asked_query, query_category, web_title, web_url, web_snippet, today])\n",
    "            # SERP_results.append([query, reformed_query, asked_query, query_category, web_title, web_url, web_snippet, today])\n",
    "    except:\n",
    "        print(query)\n",
    "        SERP_results.append([query, asked_query, query_category, None, None, None, today])\n",
    "\n",
    "SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"asked_query\", \"query_category\", \"web_title\", \"web_url\", \"web_snippet\", \"date_crawled\"])\n",
    "# SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"reformed_query\", \"asked_query\", \"query_category\", \"web_title\", \"web_url\", \"web_snippet\", \"date_crawled\"])\n",
    "SERP_df.to_excel(\"../data/Control_Google_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from datetime import datetime \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = GenerativeModel(\"gemini-pro-vision\")\n",
    "# queries_df = pd.read_excel(\"../Data/SelectedQueries_2.xlsx\")\n",
    "queries_df = pd.read_excel(\"../Data/ControlQueries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a88729cd704c7dace62bd1e1f35009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No repsonse generated for  2  queries.\n"
     ]
    }
   ],
   "source": [
    "# response = model.generate_content(\n",
    "#     [\"Hello\"]\n",
    "# )\n",
    "# print(response.candidates[0].content.parts[0].text)\n",
    "\n",
    "SERP_results = []\n",
    "# queries_df = queries_df.iloc[range(0,5)]\n",
    "today = datetime.now()\n",
    "today = today.strftime(\"%Y_%m_%d\")\n",
    "failed_query = 0\n",
    "\n",
    "for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "    query = row[\"query\"]\n",
    "    # reformed_query = \"I am autistic please explain \" +  query + \" to me\"\n",
    "    query_category = str(row[\"ngram\"]) + '-gram'\n",
    "\n",
    "    response = model.generate_content(\n",
    "        [query]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response_text = response.candidates[0].content.parts[0].text\n",
    "    except:\n",
    "        failed_query += 1\n",
    "\n",
    "    SERP_results.append([query, query_category, response_text, today])\n",
    "    # SERP_results.append([query, reformed_query, query_category, response_text, today])\n",
    "\n",
    "SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"query_category\", \"response_text\", \"date_crawled\"])\n",
    "# SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"reformed_query\", \"query_category\", \"response_text\", \"date_crawled\"])\n",
    "SERP_df.to_excel(\"../data/Control_Gemini_results.xlsx\", index=False)\n",
    "print(\"No repsonse generated for \", failed_query, \" queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# queries_df = pd.read_excel(\"../Data/SelectedQueries_2.xlsx\")\n",
    "queries_df = pd.read_excel(\"../Data/ControlQueries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc33a9748774f009adc557b079e24ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No repsonse generated for  0  queries.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from datetime import datetime \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "f = open(\"API_keys.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "API_KEY = data[\"chatgpt\"][\"api_key\"]\n",
    "f.close()\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "model_id = 'gpt-3.5-turbo'\n",
    "\n",
    "SERP_results = []\n",
    "# queries_df = queries_df.iloc[range(0,5)]\n",
    "today = datetime.now()\n",
    "today = today.strftime(\"%Y_%m_%d\")\n",
    "failed_query = 0\n",
    "\n",
    "for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "    query = row[\"query\"]\n",
    "    # reformed_query = \"I am autistic please explain \" +  query + \" to me\"\n",
    "    query_category = str(row[\"ngram\"]) + '-gram'\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "    ],\n",
    "        model=model_id,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response_text = response.choices[0].message.content\n",
    "    except:\n",
    "        failed_query += 1\n",
    "    # SERP_results.append([query, query_category, web_title, web_url, web_snippet, web_text, today])\n",
    "    SERP_results.append([query, query_category, response_text, today])\n",
    "\n",
    "# SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"query_category\", \"web_title\", \"web_url\", \"web_snippet\", \"web_text\", \"date_crawled\"])\n",
    "SERP_df = pd.DataFrame(SERP_results, columns=[\"query\", \"query_category\", \"response_text\", \"date_crawled\"])\n",
    "SERP_df.to_excel(\"../data/Control_ChatGPT_results.xlsx\", index=False)\n",
    "print(\"No repsonse generated for \", failed_query, \" queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebAccessibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
